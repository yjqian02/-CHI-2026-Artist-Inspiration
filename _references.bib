% methods
@article{braun2019reflecting,
  title={Reflecting on reflexive thematic analysis},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative research in sport, exercise and health},
  volume={11},
  number={4},
  pages={589--597},
  year={2019},
  publisher={Taylor \& Francis}
}
% misc. other/ not organized references
@inproceedings{kawakami2024impact,
  title={The impact of generative ai on artists},
  author={Kawakami, Reishiro and Venkatagiri, Sukrit},
  booktitle={Proceedings of the 16th Conference on Creativity \& Cognition},
  pages={79--82},
  year={2024}
}
@inproceedings{adams1999cognitive,
  title={Cognitive processes in iterative design behavior},
  author={Adams, Robin S and Atman, Cynthia J},
  booktitle={FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No. 99CH37011},
  volume={1},
  pages={11A6--13},
  year={1999},
  organization={IEEE}
}
@inproceedings{ko2023large,
  title={Large-scale text-to-image generation models for visual artists’ creative works},
  author={Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
  booktitle={Proceedings of the 28th international conference on intelligent user interfaces},
  pages={919--933},
  year={2023}
}
@misc{AdobeFirefly,
  author       = {{Adobe Inc.}},
  title        = {Adobe Firefly: Generative AI for Creatives},
  year         = {2025},
  howpublished = {\url{https://firefly.adobe.com}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{gallagher2017sketching,
  title={Sketching for ideation: A structured approach for increasing divergent thinking},
  author={Gallagher, Courtney Lynn},
  booktitle={Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  pages={106--111},
  year={2017}
}
@article{camba2018conceptual,
  title={Conceptual product design in digital and traditional sketching environments: a comparative exploratory study},
  author={Camba, Jorge D and Kimbrough, Mark and Kwon, EunSook},
  journal={Journal of Design Research},
  volume={16},
  number={2},
  pages={131--154},
  year={2018},
  publisher={Inderscience Publishers (IEL)}
}

@inproceedings{kong2019understanding,
  title={Understanding visual cues in visualizations accompanied by audio narrations},
  author={Kong, Ha-Kyung and Zhu, Wenjie and Liu, Zhicheng and Karahalios, Karrie},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--13},
  year={2019}
}

@inproceedings{sermuga2021uisketch,
  title={UISketch: a large-scale dataset of UI element sketches},
  author={Sermuga Pandian, Vinoth Pandian and Suleri, Sarah and Jarke, Prof Dr Matthias},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile ui summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages={498--510},
  year={2021}
}

@article{botella2018stages,
  title={What are the stages of the creative process? What visual art students are saying.},
  author={Botella, Marion and Zenasni, Franck and Lubart, Todd},
  journal={Frontiers in psychology},
  volume={9},
  pages={2266},
  year={2018},
  publisher={Frontiers Media SA}
}
@misc{Midjourney,
  author       = {{Midjourney, Inc.}},
  title        = {Midjourney AI Art Generator},
  year         = {2025},
  howpublished = {\url{https://www.midjourney.com}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{o2015designscape,
  title={Designscape: Design with interactive layout suggestions},
  author={O'Donovan, Peter and Agarwala, Aseem and Hertzmann, Aaron},
  booktitle={Proceedings of the 33rd annual ACM conference on human factors in computing systems},
  pages={1221--1224},
  year={2015}
}
@article{zhang2025generating,
  title={Generating Past and Future in Digital Painting Processes},
  author={Zhang, Lvmin and Yan, Chuan and Guo, Yuwei and Xing, Jinbo and Agrawala, Maneesh},
  journal={ACM Transactions on Graphics (TOG)},
  volume={44},
  number={4},
  pages={1--13},
  year={2025},
  publisher={ACM New York, NY, USA}
}
@misc{PaintsUNDO,
  author       = {Zhang, Lvmin},
  title        = {Paints-UNDO},
  year         = {2025},
  howpublished = {\url{https://github.com/lllyasviel/Paints-UNDO}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{liu2025magicquill,
  title={Magicquill: An intelligent interactive image editing system},
  author={Liu, Zichen and Yu, Yue and Ouyang, Hao and Wang, Qiuyu and Cheng, Ka Leong and Wang, Wen and Liu, Zhiheng and Chen, Qifeng and Shen, Yujun},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={13072--13082},
  year={2025}
}
@inproceedings{fan2024contextcam,
  title={ContextCam: Bridging context awareness with creative human-AI image co-creation},
  author={Fan, Xianzhe and Wu, Zihan and Yu, Chun and Rao, Fenggui and Shi, Weinan and Tu, Teng},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2024}
}
@inproceedings{wang2025aideation,
  title={AIdeation: Designing a Human-AI Collaborative Ideation System for Concept Designers},
  author={Wang, Wen-Fan and Lu, Chien-Ting and Ponsa i Campany{\`a}, Nil and Chen, Bing-Yu and Chen, Mike Y},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--28},
  year={2025}
}

@inproceedings{unstraighten,
author = {Taylor, Jordan and Mire, Joel and Spektor, Franchesca and DeVrio, Alicia and Sap, Maarten and Zhu, Haiyi and Fox, Sarah E},
title = {Un-Straightening Generative AI: How Queer Artists Surface and Challenge Model Normativity},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732061},
doi = {10.1145/3715275.3732061},
abstract = {Queer people are often discussed as targets of bias, harm, or discrimination in generative AI research. However, the specific ways that queer people engage with generative AI, and thus possible uses that support queer people, have yet to be explored. We conducted a workshop study with 13 queer artists, during which we gave participants access to GPT-4 and DALL-E 3 and facilitated group sensemaking activities. Our participants struggled to use these models due to various normative values embedded in their designs, such as hyper-positivity and anti-sexuality. We describe various strategies our participants developed to overcome these models’ limitations and how, nevertheless, some found value in these highly-normative technologies. Drawing on queer feminist theory, we discuss implications for the conceptualization of "state-of-the-art" models and consider how FAccT researchers might support queer alternatives.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {951–963},
numpages = {13},
location = {
},
series = {FAccT '25}
}


@article{barriers,
  title={What barriers do people experience to engaging in the arts? Structural equation modelling of the relationship between individual characteristics and capabilities, opportunities, and motivations to engage},
  author={Fancourt, Daisy and Mak, Hei Wan},
  journal={PLoS One},
  volume={15},
  number={3},
  pages={e0230487},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{zhang2023text,
  title={Text-to-image diffusion models in generative ai: A survey},
  author={Zhang, Chenshuang and Zhang, Chaoning and Zhang, Mengchun and Kweon, In So},
  journal={arXiv preprint arXiv:2303.07909},
  year={2023}
}

@article{han2024progressive,
  title={Progressive compositionality in text-to-image generative models},
  author={Han, Evans Xu and Jin, Linghao and Liu, Xiaofeng and Liang, Paul Pu},
  journal={arXiv preprint arXiv:2410.16719},
  year={2024}
}

@inproceedings{
podell2024sdxl,
title={{SDXL}: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M{\"u}ller and Joe Penna and Robin Rombach},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=di52zR8xgf}
}

@article{paradox,
  title={The paradox of artificial creativity: Challenges and opportunities of generative AI artistry},
  author={Garcia, Manuel B},
  journal={Creativity Research Journal},
  pages={1--14},
  year={2024},
  publisher={Taylor \& Francis}
}


@phdthesis{democratize,
  title={Generative AI as a Democratizing Force in Fine Arts: Navigating the New Landscape of Creation and Consumption},
  author={Jackson, Will},
  school={University of Texas at Austin},
  year={2024}
}


@book{real_benefits,
  title={Studio thinking 3: The real benefits of visual arts education},
  author={Sheridan, Kimberly M and Veenema, Shirley and Winner, Ellen and Hetland, Lois},
  year={2022},
  publisher={Teachers College Press}
}


@article{grades,
  title={The impacts of a high-school art-based program on academic achievements, creativity, and creative behaviors},
  author={Egana-delSol, Pablo},
  journal={npj Science of Learning},
  volume={8},
  number={1},
  pages={39},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{memory,
  title={Art and memory: An examination of the learning benefits of visual-art exposure},
  author={Rosier, James Tyler},
  year={2010}
}


@article{clinical,
  title={Utilizing visual art to enhance the clinical observation skills of medical students},
  author={Jasani, Sona K and Saks, Norma S},
  journal={Medical teacher},
  volume={35},
  number={7},
  pages={e1327--e1331},
  year={2013},
  publisher={Taylor \& Francis}
}

@inproceedings{foreground,
  title={Foregrounding artist opinions: A survey study on transparency, ownership, and fairness in AI generative art},
  author={Lovato, Juniper and Zimmerman, Julia Witte and Smith, Isabelle and Dodds, Peter and Karson, Jennifer L},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={905--916},
  year={2024}
}

@article{decolonial,
  title={Towards a decolonial I in AI: mapping the pervasive effects of artificial intelligence on the art ecosystem},
  author={Baradaran, Amir},
  journal={Ai \& Society},
  volume={39},
  number={1},
  pages={7--19},
  year={2024},
  publisher={Springer}
}

@inproceedings{lgtm,
author = {Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
title = {Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584078},
doi = {10.1145/3581641.3584078},
abstract = {Large-scale Text-to-image Generation Models (LTGMs) (e.g., DALL-E), self-supervised deep learning models trained on a huge dataset, have demonstrated the capacity for generating high-quality open-domain images from multi-modal input. Although they can even produce anthropomorphized versions of objects and animals, combine irrelevant concepts in reasonable ways, and give variation to any user-provided images, we witnessed such rapid technological advancement left many visual artists disoriented in leveraging LTGMs more actively in their creative works. Our goal in this work is to understand how visual artists would adopt LTGMs to support their creative works. To this end, we conducted an interview study as well as a systematic literature review of 72 system/application papers for a thorough examination. A total of 28 visual artists covering 35 distinct visual art domains acknowledged LTGMs’ versatile roles with high usability to support creative works in automating the creation process (i.e., automation), expanding their ideas (i.e., exploration), and facilitating or arbitrating in communication (i.e., mediation). We conclude by providing four design guidelines that future researchers can refer to in making intelligent user interfaces using LTGMs.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {919–933},
numpages = {15},
keywords = {DALL-E, Large-scale text-to-image generation model, interview study, literature review, visual artists},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{art_implications,
  title={Co-creating art with generative artificial intelligence: Implications for artworks and artists},
  author={Messer, Uwe},
  journal={Computers in human behavior: artificial humans},
  volume={2},
  number={1},
  pages={100056},
  year={2024},
  publisher={Elsevier}
}

@article{poet,
  title={POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation},
  author={Han, Evans Xu and Zhang, Alice Qian and Shen, Hong and Zhu, Haiyi and Liang, Paul Pu and Hsieh, Jane},
  journal={arXiv preprint arXiv:2504.13392},
  year={2025}
}

@inproceedings{lamuse,
  title={Lamuse: Leveraging artificial intelligence for sparking inspiration},
  author={Lamiroy, Bart and Potier, Emmanuelle},
  booktitle={International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)},
  pages={148--161},
  year={2022},
  organization={Springer}
}

@misc{credit,
  title={Who gets credit for AI-generated art? iScience. 2020; 23 (9): 101515},
  author={Epstein, Z and Levine, S and Rand, DG and Rahwan, I},
  year={2020},
  publisher={Article}
}


@article{versus,
  title={Humans versus AI: whether and why we prefer human-created compared to AI-created artwork},
  author={Bellaiche, Lucas and Shahi, Rohin and Turpin, Martin Harry and Ragnhildstveit, Anya and Sprockett, Shawn and Barr, Nathaniel and Christensen, Alexander and Seli, Paul},
  journal={Cognitive research: principles and implications},
  volume={8},
  number={1},
  pages={42},
  year={2023},
  publisher={Springer}
}

@article{imperial,
  title={Generative AI, Everyday Aesthetic Production, and the Imperial Mode of Living},
  author={Kemper, Jakko},
  journal={Critical AI},
  volume={3},
  number={1},
  year={2025},
  publisher={Duke University Press}
}


@article{wu2025qwen,
  title={Qwen-image technical report},
  author={Wu, Chenfei and Li, Jiahao and Zhou, Jingren and Lin, Junyang and Gao, Kaiyuan and Yan, Kun and Yin, Sheng-ming and Bai, Shuai and Xu, Xiao and Chen, Yilei and others},
  journal={arXiv preprint arXiv:2508.02324},
  year={2025}
}

@misc{
OrganizingPhoto,
title={Organizing Unstructured Image Collections using Natural Language},
author={Mingxuan Liu and Zhun Zhong and Jun Li and Gianni Franchi and Subhankar Roy and Elisa Ricci},
year={2025},
url={https://openreview.net/forum?id=PhRYDGqiee}
}

@misc{GenQuery,
      title={GenQuery: Supporting Expressive Visual Search with Generative Models}, 
      author={Kihoon Son and DaEun Choi and Tae Soo Kim and Young-Ho Kim and Juho Kim},
      year={2024},
      eprint={2310.01287},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2310.01287}, 
}

@inproceedings{wang2024promptcharm,
  title={Promptcharm: Text-to-image generation through multi-modal prompting and refinement},
  author={Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{chi2020automatic,
  title={Automatic video creation from a web page},
  author={Chi, Peggy and Sun, Zheng and Panovich, Katrina and Essa, Irfan},
  booktitle={Proceedings of the 33rd annual ACM symposium on user interface software and technology},
  pages={279--292},
  year={2020}
}

@inproceedings{chi2022synthesis,
  title={Synthesis-assisted video prototyping from a document},
  author={Chi, Peggy and Dong, Tao and Frueh, Christian and Colonna, Brian and Kwatra, Vivek and Essa, Irfan},
  booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--10},
  year={2022}
}

@article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  volume={56},
  number={10},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}

@misc{VSC,
      title={VSC: Visual Search Compositional Text-to-Image Diffusion Model}, 
      author={Do Huu Dat and Nam Hyeonu and Po-Yuan Mao and Tae-Hyun Oh},
      year={2025},
      eprint={2505.01104},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.01104}, 
}

@misc{flux,
      title={FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space}, 
      author={Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas Müller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith},
      year={2025},
      eprint={2506.15742},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2506.15742}, 
}

@misc{trellis,
      title={Structured 3D Latents for Scalable and Versatile 3D Generation}, 
      author={Jianfeng Xiang and Zelong Lv and Sicheng Xu and Yu Deng and Ruicheng Wang and Bowen Zhang and Dong Chen and Xin Tong and Jiaolong Yang},
      year={2025},
      eprint={2412.01506},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.01506}, 
}

@article{shi2023understanding,
  title={Understanding generative AI in art: an interview study with artists on G-AI from an HCI perspective},
  author={Shi, Jingyu and Jain, Rahul and Duan, Runlin and Ramani, Karthik},
  journal={arXiv preprint arXiv:2310.13149},
  year={2023}
}

@inproceedings{li2024user,
  title={User experience design professionals’ perceptions of generative artificial intelligence},
  author={Li, Jie and Cao, Hancheng and Lin, Laura and Hou, Youyang and Zhu, Ruihao and El Ali, Abdallah},
  booktitle={Proceedings of the 2024 CHI conference on human factors in computing systems},
  pages={1--18},
  year={2024}
}

@inproceedings{hu2025designing,
  title={Designing Interactions with Generative AI for Art and Creativity: A Systematic Review and Taxonomy},
  author={Hu, Xi and Xing, Yiwen and Cai, Xudong and Zhao, Yihang and Cook, Michael and Borgo, Rita and Neate, Timothy},
  booktitle={Proceedings of the 2025 ACM Designing Interactive Systems Conference},
  pages={1126--1155},
  year={2025}
}

@inproceedings{jiang2025forging,
  title={Forging an HCI Research Agenda with Artists Impacted by Generative AI},
  author={Jiang, Harry H and Agnew, William and Friedlander, Tim and Yang, Zhuolin and Fox, Sarah E and Bernstein, Michael S and Passananti, Josephine Charlie and Ogata, Megumi and Ortiz, Karla},
  booktitle={Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--4},
  year={2025}
}

@article{giacomin2023intersection,
  title={The intersection of AI and Art-Mapping the controversy around Generative AI tools as producers of artworks in the 2020s},
  author={Giacomin Da Silva, Carolina},
  year={2023}
}

@inproceedings{inie2023designing,
  title={Designing participatory ai: Creative professionals’ worries and expectations about generative ai},
  author={Inie, Nanna and Falk, Jeanette and Tanimoto, Steve},
  booktitle={Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--8},
  year={2023}
}

@inproceedings{muller2025genaichi,
  title={GenAICHI 2025: Generative AI and HCI at CHI 2025},
  author={Muller, Michael and Chilton, Lydia B and Maher, Mary Lou and Martin, Charles Patrick and Choi, Minsik and Walsh, Greg and Kantosalo, Anna},
  booktitle={Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--9},
  year={2025}
}

@article{epstein2023art,
  title={Art and the science of generative AI},
  author={Epstein, Ziv and Hertzmann, Aaron and Investigators of Human Creativity and Akten, Memo and Farid, Hany and Fjeld, Jessica and Frank, Morgan R and Groh, Matthew and Herman, Laura and Leach, Neil and others},
  journal={Science},
  volume={380},
  number={6650},
  pages={1110--1111},
  year={2023},
  publisher={American Association for the Advancement of Science}
}


@misc{magiccolor,
      title={Follow-Your-Color: Multi-Instance Sketch Colorization}, 
      author={Yinhan Zhang and Yue Ma and Bingyuan Wang and Qifeng Chen and Zeyu Wang},
      year={2025},
      eprint={2503.16948},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.16948}, 
}
@misc{blora,
      title={Implicit Style-Content Separation using B-LoRA}, 
      author={Yarden Frenkel and Yael Vinker and Ariel Shamir and Daniel Cohen-Or},
      year={2024},
      eprint={2403.14572},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.14572}, 
}
@misc{yan2025imagereferencedsketchcolorization,
      title={Image Referenced Sketch Colorization Based on Animation Creation Workflow}, 
      author={Dingkun Yan and Xinrui Wang and Zhuoru Li and Suguru Saito and Yusuke Iwasawa and Yutaka Matsuo and Jiaxian Guo},
      year={2025},
      eprint={2502.19937},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.19937}, 
}

@misc{backgroundremove,
      title={An Inpainting-Infused Pipeline for Attire and Background Replacement}, 
      author={Felipe Rodrigues Perche-Mahlow and André Felipe-Zanella and William Alberto Cruz-Castañeda and Marcellus Amadeus},
      year={2024},
      eprint={2402.03501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.03501}, 
}
@misc{gpt4v,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and etc. },
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{ati,
      title={ATI: Any Trajectory Instruction for Controllable Video Generation}, 
      author={Angtian Wang and Haibin Huang and Jacob Zhiyuan Fang and Yiding Yang and Chongyang Ma},
      year={2025},
      eprint={2505.22944},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.22944}, 
}

@article{paintsalter,
author = {Zhang, Lvmin and Yan, Chuan and Guo, Yuwei and Xing, Jinbo and Agrawala, Maneesh},
title = {Generating Past and Future in Digital Painting Processes},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3731160},
doi = {10.1145/3731160},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {127},
numpages = {13},
keywords = {digital painting, generative models, diffusion models}
}


@misc{swiftsketch,
      title={SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation}, 
      author={Ellie Arar and Yarden Frenkel and Daniel Cohen-Or and Ariel Shamir and Yael Vinker},
      year={2025},
      eprint={2502.08642},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.08642}, 
}
@misc{neutralstroke,
      title={Neural Strokes: Stylized Line Drawing of 3D Shapes}, 
      author={Difan Liu and Matthew Fisher and Aaron Hertzmann and Evangelos Kalogerakis},
      year={2021},
      eprint={2110.03900},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.03900}, 
}

@inproceedings{brooks2023instructpix2pix,
  title     = {InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author    = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023}
}

@inproceedings{haque2023instructnerf2nerf,
  title     = {Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions},
  author    = {Haque, A. and others},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2023}
}

@article{yu2023editdiffnerf,
  title   = {Editing 3D Neural Radiance Fields using 2D Diffusion Models},
  author  = {Yu, Lang and others},
  journal = {arXiv preprint arXiv:2306.09551},
  year    = {2023}
}

@article{chen2023gaussianeditor,
  title   = {GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting},
  author  = {Chen, Yiwen and others},
  journal = {arXiv preprint arXiv:2311.14521},
  year    = {2023}
}

@inproceedings{wang2024gaussianeditor,
  title     = {GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions},
  author    = {Wang, Junjie and Fang, Jiemin and Zhang, Xiaopeng and Xie, Lingxi and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024}
}

@article{poole2022dreamfusion,
  title   = {DreamFusion: Text-to-3D using 2D Diffusion},
  author  = {Poole, Ben and others},
  journal = {arXiv preprint arXiv:2209.14988},
  year    = {2022}
}

@article{lin2022magic3d,
  title   = {Magic3D: High-Resolution Text-to-3D Content Creation},
  author  = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and others},
  journal = {arXiv preprint arXiv:2211.10440},
  year    = {2022}
}

@inproceedings{wang2023prolificdreamer,
  title     = {ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation},
  author    = {Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2023}
}

@article{li2025voxhammer,
  title   = {VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space},
  author  = {Li, Lin and Huang, Zehuan and Feng, Haoran and Zhuang, Gengxiong and Chen, Rui and Guo, Chunchao and Sheng, Lu},
  journal = {arXiv preprint arXiv:2508.19247},
  year    = {2025}
}


@incollection{personal,
  title={Using the visual arts to expand personal creativity},
  author={Nadeau, Roberta},
  booktitle={Using the creative arts in therapy and healthcare},
  pages={49--71},
  year={2003},
  publisher={Routledge}
}

@article{edu, 
title={Shifting Canvases: The Evolution of Media and Techniques in Art Education with Generative AI}, volume={1}, url={https://journal.iistr.org/index.php/JARDIN/article/view/1248}, DOI={10.56741/jardin.v1i01.1248}, abstractNote={&amp;lt;p&amp;gt;Generative AI has revolutionized art education by introducing novel media and techniques,  from AI-generated art and generative design to interactive, co-creative workflows. This opinion-based article critically examines how tools like DALL·E, MidJourney, Artbreeder, and Runway ML transform artistic teaching and learning. Through a literature review, we identify pedagogical implications, democratization effects, and ethical considerations. Our results, presented in structured tables, highlight emerging opportunities and tensions. We argue that AI serves as both a disruptor and enhancer, necessitating updated curriculum frameworks that embrace creative agency and critical literacy.&amp;lt;/p&amp;gt;}, number={01}, journal={The Journal of Art and Design Innovation}, author={Arslan, Aysel and Probosiwi}, year={2025}, month={Jul.}, pages={51–59} }

@Inbook{flow,
author="Chemi, Tatiana",
editor="Harmat, L{\'a}szl{\'o}
and {\O}rsted Andersen, Frans
and Ull{\'e}n, Fredrik
and Wright, Jon
and Sadlo, Gaynor",
title="The Experience of Flow in Artistic Creation",
bookTitle="Flow Experience: Empirical Research and Applications",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="37--50",
abstract="In the present chapter I will describe and discuss the experience of flow occurring when professional artists create and learn. The unique findings are part of a larger study that looked at artistic creativity and aimed at describing the ways in which professional and widely recognised artists create, learn and organise their work. The methodological approach was qualitative and based on retrospective narratives of 22 high-achieving professional artists. When asked to describe the ways in which they create, these artists often replied by mentioning a positively felt state of deep concentration and calm. Flow, to them, seems to be at the same time a pre-requisite, a constitutive element and an effect of creative processes. Artistic creation happens in a state of deep concentration and self-forgetting and flow seems to have a specific purpose within artistic processes: triggering, facilitating and guiding the flow of creation.",
isbn="978-3-319-28634-1",
doi="10.1007/978-3-319-28634-1_3",
url="https://doi.org/10.1007/978-3-319-28634-1_3"
}



@article{multivariate,
title = {How artists create: Creative process and multivariate factors},
journal = {Learning and Individual Differences},
volume = {26},
pages = {161-170},
year = {2013},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2013.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1041608013000356},
author = {Marion Botella and Vlad Glaveanu and Franck Zenasni and Martin Storme and Nils Myszkowski and Marion Wolff and Todd Lubart},
keywords = {Creativity, Artists, Process, Semantic analysis, Activity theory},
abstract = {This study sought to identify the factors that artists consider important for their creativity and to reconstruct, from interviews, the stages of their creative activity. For this purpose, 27 interviews with professional artists were analyzed using a double approach. First, a quantitative analysis of interviews and associated self-report questionnaires was performed. Second, a qualitative coding grid was applied to a representative subset of the interviews to uncover stages of activity and the interaction between creator and the material and social world. Results are discussed according to the multivariate approach and in light of activity theory and its emphasis on situated, goal-directed and meaningful action. Findings concerning the creative process and the factors involved are finally considered with respect to teaching creativity and art.}
}

@article{thought,
author = {Catharine Patrick},
title = {Creative thought in Artists},
journal = {The Journal of Psychology},
volume = {4},
number = {1},
pages = {35--73},
year = {1937},
publisher = {Routledge},
doi = {10.1080/00223980.1937.9917525},
URL = { 
        https://doi.org/10.1080/00223980.1937.9917525
},
eprint = { 
        https://doi.org/10.1080/00223980.1937.9917525
}
}

@article{wallas,
  title={The art of thought},
  author={Wallas, G},
  journal={Jonathan Cape},
  year={1926}
}

@article{wallas_followup,
  title={Wallas’ four-stage model of the creative process: More than meets the eye?},
  author={Sadler-Smith, Eugene},
  journal={Creativity research journal},
  volume={27},
  number={4},
  pages={342--352},
  year={2015},
  publisher={Taylor \& Francis}
}

@book{geneplore,
  title={Creative cognition: Theory, research, and applications},
  author={Finke, Ronald A and Ward, Thomas B and Smith, Steven M},
  year={1996},
  publisher={MIT press}
}

@article{vision,
  title={The creative vision: A longitudinal study of problem finding in art},
  author={Getzels, Jacob W and Csikszentmihalyi, Mihaly},
  journal={(No Title)},
  year={1976}
}

@article{rhodes,
  title={An analysis of creativity},
  author={Rhodes, Mel},
  journal={The Phi delta kappan},
  volume={42},
  number={7},
  pages={305--310},
  year={1961},
  publisher={JSTOR}
}

@article{soi,
  title={Guilford's structure of intellect model: its relevance for the teacher preparation curriculum},
  author={Edwards, Reginald},
  journal={Curriculum Theory Network},
  volume={1},
  number={3},
  pages={47--64},
  year={1969},
  publisher={Taylor \& Francis}
}

@article{modeling,
  title={Modeling the creative process: A grounded theory analysis of creativity in the domain of art making},
  author={Mace, Mary-Anne and Ward, Tony},
  journal={Creativity research journal},
  volume={14},
  number={2},
  pages={179--192},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{solving,
  title={The figural problem solving and problem finding of professional and semiprofessional artists and nonartists},
  author={Kay, Sandra},
  journal={Creativity Research Journal},
  volume={4},
  number={3},
  pages={233--252},
  year={1991},
  publisher={Taylor \& Francis}
}

@article{modification,
  title={The process of art-making and creative expertise: An analysis of artists' process modification},
  author={Yokochi, Sawako and Okada, Takeshi},
  journal={The Journal of Creative Behavior},
  volume={55},
  number={2},
  pages={532--545},
  year={2021},
  publisher={Wiley Online Library}
}

@article{educational,
  title={Educational intervention and the development of young art students' talent and creativity},
  author={Rostan, Susan M},
  journal={The Journal of Creative Behavior},
  volume={39},
  number={4},
  pages={237--261},
  year={2005},
  publisher={Wiley Online Library}
}

@article{control,
  title={A phenomenology of artistic doing: Flow as embodied knowing in 2D and 3D professional artists},
  author={Banfield, Janet and Burgess, Mark},
  journal={Journal of Phenomenological Psychology},
  volume={44},
  number={1},
  pages={60--91},
  year={2013},
  publisher={Brill}
}

@article{ecological,
  title={A dynamic and ecological approach to the artistic creative process of arts students: An empirical contribution},
  author={Botella, Marion and Zenasni, Franck and Lubart, Todd},
  journal={Empirical studies of the arts},
  volume={29},
  number={1},
  pages={17--38},
  year={2011},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{subproccess,
  title={Creative Subprocess Frequencies and Their Relation to Personal Characteristics and Product Creativity: Insights from a Drawing Task Think Aloud Study},
  author={Boldt, Gregory T and Kaufman, James C},
  journal={The Journal of Creative Behavior},
  volume={59},
  number={1},
  pages={e629},
  year={2025},
  publisher={Wiley Online Library}
}

@article{mfa,
  title={How artists create: An empirical study of MFA painting students},
  author={Sawyer, R Keith},
  journal={The Journal of Creative Behavior},
  volume={52},
  number={2},
  pages={127--141},
  year={2018},
  publisher={Wiley Online Library}
}

@article{osborn,
  title={Applied imagination 3”’Edition},
  author={Osborn, AF},
  journal={New York: Charles Scribner},
  year={1963}
}

@article{busse,
  title={Theories of the creative process: a review and a perspective.},
  author={Busse, Thomas V and Mansfield, Richard S},
  journal={The Journal of Creative Behavior},
  year={1980},
  publisher={Creative Education Foundation}
}


@book{encyclopedia,
  title={Encyclopedia of creativity},
  author={Runco, Mark A and Pritzker, Steven R},
  year={2020},
  publisher={Academic press}
}

@book{torrance,
  title={Guiding creative talent},
  author={Torrance, E Paul},
  year={2018},
  publisher={Pickle Partners Publishing}
}

@book{bruford,
  title={Making it Work: Creative music performance and the Western kit drummer},
  author={Bruford, William},
  year={2016},
  publisher={University of Surrey (United Kingdom)}
}

@article{furst,
  title={The creative process in visual art: A longitudinal multivariate study},
  author={F{\"u}rst, Guillaume and Ghisletta, Paolo and Lubart, Todd},
  journal={Creativity Research Journal},
  volume={24},
  number={4},
  pages={283--295},
  year={2012},
  publisher={Taylor \& Francis}
}

@book{boden,
  title={The creative mind: Myths and mechanisms},
  author={Boden, Margaret A},
  year={2004},
  publisher={Routledge}
}

@inproceedings{centric,
author = {Jacobs, Jennifer and Brandt, Joel and Mech, Radom\'{\i}r and Resnick, Mitchel},
title = {Extending Manual Drawing Practices with Artist-Centric Programming Tools},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174164},
doi = {10.1145/3173574.3174164},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {generative art, procedural art, programming},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{PortraitSketch,
author = {Xie, Jun and Hertzmann, Aaron and Li, Wilmot and Winnem\"{o}ller, Holger},
title = {PortraitSketch: face sketching assistance for novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647399},
doi = {10.1145/2642918.2647399},
abstract = {We present PortraitSketch, an interactive drawing system that helps novices create pleasing, recognizable face sketches without requiring prior artistic training. As the user traces over a source portrait photograph, PortraitSketch automatically adjusts the geometry and stroke parameters (thickness, opacity, etc.) to improve the aesthetic quality of the sketch. We present algorithms for adjusting both outlines and shading strokes based on important features of the underlying source image. In contrast to automatic stylization systems, PortraitSketch is designed to encourage a sense of ownership and accomplishment in the user. To this end, all adjustments are performed in real-time, and the user ends up directly drawing all strokes on the canvas. The findings from our user study suggest that users prefer drawing with some automatic assistance, thereby producing better drawings, and that assistance does not decrease the perceived level of involvement in the creative process.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {407–417},
numpages = {11},
keywords = {creativity support tool, novices, portraits, sketching},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{k_sketch,
author = {Davis, Richard C. and Colwell, Brien and Landay, James A.},
title = {K-sketch: a 'kinetic' sketch pad for novice animators},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357122},
doi = {10.1145/1357054.1357122},
abstract = {Because most animation tools are complex and time-consuming to learn and use, most animations today are created by experts. To help novices create a wide range of animations quickly, we have developed a general-purpose, informal, 2D animation sketching system called K-Sketch. Field studies investigating the needs of animators and would-be animators helped us collect a library of usage scenarios for our tool. A novel optimization technique enabled us to design an interface that is simultaneously fast, simple, and powerful. The result is a pen-based system that relies on users' intuitive sense of space and time while still supporting a wide range of uses. In a laboratory experiment that compared K-Sketch to a more formal animation tool (PowerPoint), participants worked three times faster, needed half the learning time, and had significantly lower cognitive load with K-Sketch.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {413–422},
numpages = {10},
keywords = {sketching, pen-based user interfaces, informal user interfaces, animation},
location = {Florence, Italy},
series = {CHI '08}
}

@inproceedings{bob,
author = {Benedetti, Luca and Winnem\"{o}ller, Holger and Corsini, Massimiliano and Scopigno, Roberto},
title = {Painting with Bob: assisted creativity for novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647415},
doi = {10.1145/2642918.2647415},
abstract = {Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {419–428},
numpages = {10},
keywords = {creativity, novices, painting},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}
@inproceedings{lin2014visual,
  title={Visual semantic search: Retrieving videos via complex textual queries},
  author={Lin, Dahua and Fidler, Sanja and Kong, Chen and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2657--2664},
  year={2014}
}

@article{cocreative,
  title={Exploring co-creative drawing workflows},
  author={Jansen, Chipp and Sklar, Elizabeth},
  journal={Frontiers in Robotics and AI},
  volume={8},
  pages={577770},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{devalue,
  title={Co-creating art with generative artificial intelligence: Implications for artistic evaluation and collaboration dynamics in the digital age},
  author={Zhao, Y and Liu, J},
  journal={Artificial Intelligence Review},
  year={2024}
}

@article{image_democratization,
  title={Democratization and generative AI image creation: aesthetics, citizenship, and practices},
  author={Bak Herrie, Maja and Maleve, Nicolas Ren{\'e} and Philipsen, Lotte and Staun{\ae}s, Asker Bryld},
  journal={AI \& SOCIETY},
  volume={40},
  number={5},
  pages={3495--3507},
  year={2025},
  publisher={Springer}
}

@article{decline_novelty,
    author = {Zhou, Eric and Lee, Dokyun},
    title = {Generative artificial intelligence, human creativity, and art},
    journal = {PNAS Nexus},
    volume = {3},
    number = {3},
    pages = {52},
    year = {2024},
    month = {03},
    abstract = {Recent artificial intelligence (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image generative AI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans’ artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25\% and increases the value as measured by the likelihood of receiving a favorite per view by 50\%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to “generative synesthesia”—the harmonious blending of human exploration and AI exploitation to discover new creative workflows.},
    issn = {2752-6542},
}


@article{student_ai,
  title={My teammate is AI: Understanding students’ perceptions of student-AI collaboration in drawing tasks},
  author={Kim, Jinhee and Cho, Young Hoan},
  journal={Asia Pacific Journal of Education},
  volume={45},
  number={3},
  pages={1013--1027},
  year={2025},
  publisher={Taylor \& Francis}
}


@article{liberation,
	abstract = {This paper investigates the transformative influence of generative AI on the arts, connecting it with Walter Benjamin's insights regarding the aura of art in the mechanical reproduction era. It scrutinizes how generative AI not only redefines art's traditional aura but also introduces a dynamic interplay between technological liberation and dependency. The analysis extends to the democratization of artistic expression and its broader societal impacts, highlighting a shift in art creation, perception, and interpretation in the digital age. This research encapsulates the evolving nature of authenticity, creativity, and human--machine collaboration, marking a significant paradigm shift in art's conception and its role in society.},
	author = {Park, Sungjin},
	date = {2025/03/01},
	date-added = {2025-09-10 09:13:32 -0400},
	date-modified = {2025-09-10 09:13:32 -0400},
	doi = {10.1007/s00146-024-01948-6},
	id = {Park2025},
	isbn = {1435-5655},
	journal = {AI \& SOCIETY},
	number = {3},
	pages = {1807--1816},
	title = {The work of art in the age of generative AI: aura, liberation, and democratization},
	url = {https://doi.org/10.1007/s00146-024-01948-6},
	volume = {40},
	year = {2025},
	bdsk-url-1 = {https://doi.org/10.1007/s00146-024-01948-6}}


@inproceedings{everybody_sketch,
author = {Bae, Seok-Hyung and Balakrishnan, Ravin and Singh, Karan},
title = {EverybodyLovesSketch: 3D sketching for a broader audience},
year = {2009},
isbn = {9781605587455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1622176.1622189},
doi = {10.1145/1622176.1622189},
abstract = {We present EverybodyLovesSketch, a gesture-based 3D curve sketching system for rapid ideation and visualization of 3D forms, aimed at a broad audience. We first analyze traditional perspective drawing in professional practice. We then design a system built upon the paradigm of ILoveSketch, a 3D curve drawing system for design professionals. The new system incorporates many interaction aspects of perspective drawing with judicious automation to enable novices with no perspective training to proficiently create 3D curve sketches. EverybodyLovesSketch supports a number of novel interactions: tick-based sketch plane selection, single view definition of arbitrary extrusion vectors, multiple extruded surface sketching, copy-and-project of 3D curves, freeform surface sketching, and an interactive perspective grid. Finally, we present a study involving 49 high school students (with no formal artistic training) who each learned and used the system over 11 days, which provides detailed insights into the popularity, power and usability of the various techniques, and shows our system to be easily learnt and effectively used, with broad appeal.},
booktitle = {Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology},
pages = {59–68},
numpages = {10},
keywords = {sketch surface, perspective sketching, learnability, gestural interface, axis widget, 3D sketching},
location = {Victoria, BC, Canada},
series = {UIST '09}
}

@article{imperial,
  title={Generative AI, Everyday Aesthetic Production, and the Imperial Mode of Living},
  author={Kemper, Jakko},
  journal={Critical AI},
  volume={3},
  number={1},
  year={2025},
  publisher={Duke University Press}
}

@inproceedings{design,
author = {Lau, Tatiana and Carter, Scott and Chen, Francine and Huynh, Brandon and Kimani, Everlyne and Lee, Matthew L and Sieck, Kate A},
title = {Democratizing Design through Generative AI},
year = {2024},
isbn = {9798400706325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656156.3663703},
doi = {10.1145/3656156.3663703},
abstract = {Conversations around public spaces are fractured and often circle around designs and their implications for the space. Contributing to this problem is the fact that people often talk past one another. Recent advances in generative AI may help to democratize this process of designing for public spaces and enable people to meaningfully converse with one another. Here, we develop a platform that asks users to submit a photo and story about a place in their community that they cherish. The system then pairs each user with a target person whose preferences conflict with their own preferences. Users read the target person’s account and use our generative AI system to redesign a space to accommodate the target person. Preliminary studies demonstrate that the act of redesigning the space may lead to greater empathy for the target person, which may help advance conversations around public spaces.},
booktitle = {Companion Publication of the 2024 ACM Designing Interactive Systems Conference},
pages = {239–244},
numpages = {6},
keywords = {Design, Generative AI, Multi-stakeholder systems, Social connection},
location = {IT University of Copenhagen, Denmark},
series = {DIS '24 Companion}
}

@misc{dialoguemachine,
      title={Dialogue with the Machine and Dialogue with the Art World: Evaluating Generative AI for Culturally-Situated Creativity}, 
      author={Rida Qadri and Piotr Mirowski and Aroussiak Gabriellan and Farbod Mehr and Huma Gupta and Pamela Karimi and Remi Denton},
      year={2024},
      eprint={2412.14077},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2412.14077}, 
}
@book{imperiallife,
title = "The Imperial Mode of Living. Everyday Life and the Ecological Crisis of Capitalism.",
author = "Ulrich Brand and Markus Wissen",
year = "2021",
language = "English",
publisher = "Verso",
}

@inproceedings{lima,
  title={Public Opinions About Copyright for AI-Generated Art: The Role of Egocentricity, Competition, and Experience},
  author={Lima, Gabriel and Grgi{\'c}-Hla{\v{c}}a, Nina and Redmiles, Elissa M},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--32},
  year={2025}
}

@article{chatterjee,
  title={Art in an age of artificial intelligence},
  author={Chatterjee, Anjan},
  journal={Frontiers in psychology},
  volume={13},
  pages={1024449},
  year={2022},
  publisher={Frontiers Media SA}
}

@article{architectural,
  title={Using text-to-image generation for architectural design ideation},
  author={Paananen, Ville and Oppenlaender, Jonas and Visuri, Aku},
  journal={International Journal of Architectural Computing},
  volume={22},
  number={3},
  pages={458--474},
  year={2024},
  publisher={Sage publications Sage UK: London, England}
}

@phdthesis{jackson,
  title={Generative AI as a Democratizing Force in Fine Arts: Navigating the New Landscape of Creation and Consumption},
  author={Jackson, Will},
  school={University of Texas at Austin},
  year={2024}
}

@inproceedings{design_professionals,
  title={User experience design professionals’ perceptions of generative artificial intelligence},
  author={Li, Jie and Cao, Hancheng and Lin, Laura and Hou, Youyang and Zhu, Ruihao and El Ali, Abdallah},
  booktitle={Proceedings of the 2024 CHI conference on human factors in computing systems},
  pages={1--18},
  year={2024}
}

@article{shi2023understanding,
  title={Understanding generative AI in art: an interview study with artists on G-AI from an HCI perspective},
  author={Shi, Jingyu and Jain, Rahul and Duan, Runlin and Ramani, Karthik},
  journal={arXiv preprint arXiv:2310.13149},
  year={2023}
}

@article{explorers,
  title={" Explorers of Unknown Planets" Practices and Politics of Artificial Intelligence in Visual Arts},
  author={Caramiaux, Baptiste and Fdili Alaoui, Sarah},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--24},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{machine,
  title={Machine learning processes as sources of ambiguity: Insights from ai art},
  author={Sivertsen, Christian and Salimbeni, Guido and L{\o}vlie, Anders Sundnes and Benford, Steven David and Zhu, Jichen},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2024}
}
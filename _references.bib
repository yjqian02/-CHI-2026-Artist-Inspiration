% methods
@article{braun2019reflecting,
  title={Reflecting on reflexive thematic analysis},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative research in sport, exercise and health},
  volume={11},
  number={4},
  pages={589--597},
  year={2019},
  publisher={Taylor \& Francis}
}
% misc. other/ not organized references
@inproceedings{kawakami2024impact,
  title={The impact of generative ai on artists},
  author={Kawakami, Reishiro and Venkatagiri, Sukrit},
  booktitle={Proceedings of the 16th Conference on Creativity \& Cognition},
  pages={79--82},
  year={2024}
}
@inproceedings{adams1999cognitive,
  title={Cognitive processes in iterative design behavior},
  author={Adams, Robin S and Atman, Cynthia J},
  booktitle={FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No. 99CH37011},
  volume={1},
  pages={11A6--13},
  year={1999},
  organization={IEEE}
}
@inproceedings{ko2023large,
  title={Large-scale text-to-image generation models for visual artists’ creative works},
  author={Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
  booktitle={Proceedings of the 28th international conference on intelligent user interfaces},
  pages={919--933},
  year={2023}
}
@misc{AdobeFirefly,
  author       = {{Adobe Inc.}},
  title        = {Adobe Firefly: Generative AI for Creatives},
  year         = {2025},
  howpublished = {\url{https://firefly.adobe.com}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{gallagher2017sketching,
  title={Sketching for ideation: A structured approach for increasing divergent thinking},
  author={Gallagher, Courtney Lynn},
  booktitle={Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  pages={106--111},
  year={2017}
}
@article{camba2018conceptual,
  title={Conceptual product design in digital and traditional sketching environments: a comparative exploratory study},
  author={Camba, Jorge D and Kimbrough, Mark and Kwon, EunSook},
  journal={Journal of Design Research},
  volume={16},
  number={2},
  pages={131--154},
  year={2018},
  publisher={Inderscience Publishers (IEL)}
}

@inproceedings{kong2019understanding,
  title={Understanding visual cues in visualizations accompanied by audio narrations},
  author={Kong, Ha-Kyung and Zhu, Wenjie and Liu, Zhicheng and Karahalios, Karrie},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--13},
  year={2019}
}

@inproceedings{sermuga2021uisketch,
  title={UISketch: a large-scale dataset of UI element sketches},
  author={Sermuga Pandian, Vinoth Pandian and Suleri, Sarah and Jarke, Prof Dr Matthias},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile ui summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages={498--510},
  year={2021}
}

@article{botella2018stages,
  title={What are the stages of the creative process? What visual art students are saying.},
  author={Botella, Marion and Zenasni, Franck and Lubart, Todd},
  journal={Frontiers in psychology},
  volume={9},
  pages={2266},
  year={2018},
  publisher={Frontiers Media SA}
}
@misc{Midjourney,
  author       = {{Midjourney, Inc.}},
  title        = {Midjourney AI Art Generator},
  year         = {2025},
  howpublished = {\url{https://www.midjourney.com}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{o2015designscape,
  title={Designscape: Design with interactive layout suggestions},
  author={O'Donovan, Peter and Agarwala, Aseem and Hertzmann, Aaron},
  booktitle={Proceedings of the 33rd annual ACM conference on human factors in computing systems},
  pages={1221--1224},
  year={2015}
}
@article{zhang2025generating,
  title={Generating Past and Future in Digital Painting Processes},
  author={Zhang, Lvmin and Yan, Chuan and Guo, Yuwei and Xing, Jinbo and Agrawala, Maneesh},
  journal={ACM Transactions on Graphics (TOG)},
  volume={44},
  number={4},
  pages={1--13},
  year={2025},
  publisher={ACM New York, NY, USA}
}
@misc{PaintsUNDO,
  author       = {Zhang, Lvmin},
  title        = {Paints-UNDO},
  year         = {2025},
  howpublished = {\url{https://github.com/lllyasviel/Paints-UNDO}},
  note         = {Accessed: 2025-08-18}
}
@inproceedings{liu2025magicquill,
  title={Magicquill: An intelligent interactive image editing system},
  author={Liu, Zichen and Yu, Yue and Ouyang, Hao and Wang, Qiuyu and Cheng, Ka Leong and Wang, Wen and Liu, Zhiheng and Chen, Qifeng and Shen, Yujun},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={13072--13082},
  year={2025}
}
@inproceedings{fan2024contextcam,
  title={ContextCam: Bridging context awareness with creative human-AI image co-creation},
  author={Fan, Xianzhe and Wu, Zihan and Yu, Chun and Rao, Fenggui and Shi, Weinan and Tu, Teng},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2024}
}
@inproceedings{wang2025aideation,
  title={AIdeation: Designing a Human-AI Collaborative Ideation System for Concept Designers},
  author={Wang, Wen-Fan and Lu, Chien-Ting and Ponsa i Campany{\`a}, Nil and Chen, Bing-Yu and Chen, Mike Y},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--28},
  year={2025}
}

@inproceedings{unstraighten,
author = {Taylor, Jordan and Mire, Joel and Spektor, Franchesca and DeVrio, Alicia and Sap, Maarten and Zhu, Haiyi and Fox, Sarah E},
title = {Un-Straightening Generative AI: How Queer Artists Surface and Challenge Model Normativity},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732061},
doi = {10.1145/3715275.3732061},
abstract = {Queer people are often discussed as targets of bias, harm, or discrimination in generative AI research. However, the specific ways that queer people engage with generative AI, and thus possible uses that support queer people, have yet to be explored. We conducted a workshop study with 13 queer artists, during which we gave participants access to GPT-4 and DALL-E 3 and facilitated group sensemaking activities. Our participants struggled to use these models due to various normative values embedded in their designs, such as hyper-positivity and anti-sexuality. We describe various strategies our participants developed to overcome these models’ limitations and how, nevertheless, some found value in these highly-normative technologies. Drawing on queer feminist theory, we discuss implications for the conceptualization of "state-of-the-art" models and consider how FAccT researchers might support queer alternatives.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {951–963},
numpages = {13},
location = {
},
series = {FAccT '25}
}

@inproceedings{review,
author = {Hu, Xi and Xing, Yiwen and Cai, Xudong and Zhao, Yihang and Cook, Michael and Borgo, Rita and Neate, Timothy},
title = {Designing Interactions with Generative AI for Art and Creativity: A Systematic Review and Taxonomy},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735843},
doi = {10.1145/3715336.3735843},
abstract = {Generative Artificial Intelligence (GenAI) applications in artistic and creative domains have gained substantial attention of late. These intelligent interactive systems, shaped by innovations in Large Language Models (LLMs) and Vision Language Models (VLMs), are materially impacting digital creative domains. While initial work to understand this space has highlighted new models and architectures, we lack a holistic view of how interactive GenAI systems are designed for user interactions across various artistic and creative domains. In this paper, we present a systematic review of interactive GenAI system designs for art and creativity in the HCI literature (N = 189), and a detailed taxonomy of interaction paradigms with design components. We shed light on the communities of design focus and decompose the system interaction designs, mapping these characteristics to creative domains, user interaction patterns, GenAI technologies, detailing under-represented spaces, and future directions of designing interactions for GenAI creativity.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {1126–1155},
numpages = {30},
keywords = {Generative AI, art, creativity, systematic review, taxonomy.},
location = {
},
series = {DIS '25}
}

@article{barriers,
  title={What barriers do people experience to engaging in the arts? Structural equation modelling of the relationship between individual characteristics and capabilities, opportunities, and motivations to engage},
  author={Fancourt, Daisy and Mak, Hei Wan},
  journal={PLoS One},
  volume={15},
  number={3},
  pages={e0230487},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{zhang2023text,
  title={Text-to-image diffusion models in generative ai: A survey},
  author={Zhang, Chenshuang and Zhang, Chaoning and Zhang, Mengchun and Kweon, In So},
  journal={arXiv preprint arXiv:2303.07909},
  year={2023}
}

@article{han2024progressive,
  title={Progressive compositionality in text-to-image generative models},
  author={Han, Evans Xu and Jin, Linghao and Liu, Xiaofeng and Liang, Paul Pu},
  journal={arXiv preprint arXiv:2410.16719},
  year={2024}
}

@inproceedings{
podell2024sdxl,
title={{SDXL}: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M{\"u}ller and Joe Penna and Robin Rombach},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=di52zR8xgf}
}

@article{synesthesia,
  title={Generative artificial intelligence, human creativity, and art},
  author={Zhou, Eric and Lee, Dokyun},
  journal={PNAS nexus},
  volume={3},
  number={3},
  pages={pgae052},
  year={2024},
  publisher={Oxford University Press US}
}


@article{paradox,
  title={The paradox of artificial creativity: Challenges and opportunities of generative AI artistry},
  author={Garcia, Manuel B},
  journal={Creativity Research Journal},
  pages={1--14},
  year={2024},
  publisher={Taylor \& Francis}
}


@phdthesis{democratize,
  title={Generative AI as a Democratizing Force in Fine Arts: Navigating the New Landscape of Creation and Consumption},
  author={Jackson, Will},
  school={University of Texas at Austin},
  year={2024}
}


@book{real_benefits,
  title={Studio thinking 3: The real benefits of visual arts education},
  author={Sheridan, Kimberly M and Veenema, Shirley and Winner, Ellen and Hetland, Lois},
  year={2022},
  publisher={Teachers College Press}
}


@article{grades,
  title={The impacts of a high-school art-based program on academic achievements, creativity, and creative behaviors},
  author={Egana-delSol, Pablo},
  journal={npj Science of Learning},
  volume={8},
  number={1},
  pages={39},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{memory,
  title={Art and memory: An examination of the learning benefits of visual-art exposure},
  author={Rosier, James Tyler},
  year={2010}
}


@article{clinical,
  title={Utilizing visual art to enhance the clinical observation skills of medical students},
  author={Jasani, Sona K and Saks, Norma S},
  journal={Medical teacher},
  volume={35},
  number={7},
  pages={e1327--e1331},
  year={2013},
  publisher={Taylor \& Francis}
}

@inproceedings{foreground,
  title={Foregrounding artist opinions: A survey study on transparency, ownership, and fairness in AI generative art},
  author={Lovato, Juniper and Zimmerman, Julia Witte and Smith, Isabelle and Dodds, Peter and Karson, Jennifer L},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={905--916},
  year={2024}
}

@article{decolonial,
  title={Towards a decolonial I in AI: mapping the pervasive effects of artificial intelligence on the art ecosystem},
  author={Baradaran, Amir},
  journal={Ai \& Society},
  volume={39},
  number={1},
  pages={7--19},
  year={2024},
  publisher={Springer}
}

@inproceedings{impact,
author = {Kawakami, Reishiro and Venkatagiri, Sukrit},
title = {The Impact of Generative AI on Artists},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3664263},
doi = {10.1145/3635636.3664263},
abstract = {Generative AI has the potential to augment artists’ creative expression, while simultaneously harming their professions through unethical data collection practices and replacement of human labor. We conducted a thematic analysis of social media posts to understand artists’ perceptions and experiences of the direct and indirect impact of generative AI on their profession. Our findings also highlight growing public distrust toward artists amidst the rise of generative AI, with accusations of using AI tools leading to stress and fear of unemployment. Our study provides valuable insights into the complex interplay between artists, generative AI, and the public. We discuss potential protective measures for artists, including regulatory interventions and opt-in/out data collection, and explore future impacts of generative AI on artists’ creative processes.},
booktitle = {Proceedings of the 16th Conference on Creativity \& Cognition},
pages = {79–82},
numpages = {4},
keywords = {AI art, Artists, Generative AI, artwork, creative professions},
location = {Chicago, IL, USA},
series = {C&C '24}
}

@inproceedings{lgtm,
author = {Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
title = {Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584078},
doi = {10.1145/3581641.3584078},
abstract = {Large-scale Text-to-image Generation Models (LTGMs) (e.g., DALL-E), self-supervised deep learning models trained on a huge dataset, have demonstrated the capacity for generating high-quality open-domain images from multi-modal input. Although they can even produce anthropomorphized versions of objects and animals, combine irrelevant concepts in reasonable ways, and give variation to any user-provided images, we witnessed such rapid technological advancement left many visual artists disoriented in leveraging LTGMs more actively in their creative works. Our goal in this work is to understand how visual artists would adopt LTGMs to support their creative works. To this end, we conducted an interview study as well as a systematic literature review of 72 system/application papers for a thorough examination. A total of 28 visual artists covering 35 distinct visual art domains acknowledged LTGMs’ versatile roles with high usability to support creative works in automating the creation process (i.e., automation), expanding their ideas (i.e., exploration), and facilitating or arbitrating in communication (i.e., mediation). We conclude by providing four design guidelines that future researchers can refer to in making intelligent user interfaces using LTGMs.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {919–933},
numpages = {15},
keywords = {DALL-E, Large-scale text-to-image generation model, interview study, literature review, visual artists},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{art_implications,
  title={Co-creating art with generative artificial intelligence: Implications for artworks and artists},
  author={Messer, Uwe},
  journal={Computers in human behavior: artificial humans},
  volume={2},
  number={1},
  pages={100056},
  year={2024},
  publisher={Elsevier}
}

@article{poet,
  title={POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation},
  author={Han, Evans Xu and Zhang, Alice Qian and Shen, Hong and Zhu, Haiyi and Liang, Paul Pu and Hsieh, Jane},
  journal={arXiv preprint arXiv:2504.13392},
  year={2025}
}

@inproceedings{lamuse,
  title={Lamuse: Leveraging artificial intelligence for sparking inspiration},
  author={Lamiroy, Bart and Potier, Emmanuelle},
  booktitle={International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar)},
  pages={148--161},
  year={2022},
  organization={Springer}
}

@misc{credit,
  title={Who gets credit for AI-generated art? iScience. 2020; 23 (9): 101515},
  author={Epstein, Z and Levine, S and Rand, DG and Rahwan, I},
  year={2020},
  publisher={Article}
}


@article{versus,
  title={Humans versus AI: whether and why we prefer human-created compared to AI-created artwork},
  author={Bellaiche, Lucas and Shahi, Rohin and Turpin, Martin Harry and Ragnhildstveit, Anya and Sprockett, Shawn and Barr, Nathaniel and Christensen, Alexander and Seli, Paul},
  journal={Cognitive research: principles and implications},
  volume={8},
  number={1},
  pages={42},
  year={2023},
  publisher={Springer}
}

@article{imperial,
  title={Generative AI, Everyday Aesthetic Production, and the Imperial Mode of Living},
  author={Kemper, Jakko},
  journal={Critical AI},
  volume={3},
  number={1},
  year={2025},
  publisher={Duke University Press}
}


@article{wu2025qwen,
  title={Qwen-image technical report},
  author={Wu, Chenfei and Li, Jiahao and Zhou, Jingren and Lin, Junyang and Gao, Kaiyuan and Yan, Kun and Yin, Sheng-ming and Bai, Shuai and Xu, Xiao and Chen, Yilei and others},
  journal={arXiv preprint arXiv:2508.02324},
  year={2025}
}

@misc{
OrganizingPhoto,
title={Organizing Unstructured Image Collections using Natural Language},
author={Mingxuan Liu and Zhun Zhong and Jun Li and Gianni Franchi and Subhankar Roy and Elisa Ricci},
year={2025},
url={https://openreview.net/forum?id=PhRYDGqiee}
}

@misc{GenQuery,
      title={GenQuery: Supporting Expressive Visual Search with Generative Models}, 
      author={Kihoon Son and DaEun Choi and Tae Soo Kim and Young-Ho Kim and Juho Kim},
      year={2024},
      eprint={2310.01287},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2310.01287}, 
}

@inproceedings{wang2024promptcharm,
  title={Promptcharm: Text-to-image generation through multi-modal prompting and refinement},
  author={Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{chi2020automatic,
  title={Automatic video creation from a web page},
  author={Chi, Peggy and Sun, Zheng and Panovich, Katrina and Essa, Irfan},
  booktitle={Proceedings of the 33rd annual ACM symposium on user interface software and technology},
  pages={279--292},
  year={2020}
}

@inproceedings{chi2022synthesis,
  title={Synthesis-assisted video prototyping from a document},
  author={Chi, Peggy and Dong, Tao and Frueh, Christian and Colonna, Brian and Kwatra, Vivek and Essa, Irfan},
  booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--10},
  year={2022}
}

@article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  volume={56},
  number={10},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}

@misc{VSC,
      title={VSC: Visual Search Compositional Text-to-Image Diffusion Model}, 
      author={Do Huu Dat and Nam Hyeonu and Po-Yuan Mao and Tae-Hyun Oh},
      year={2025},
      eprint={2505.01104},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.01104}, 
}

@misc{flux,
      title={FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space}, 
      author={Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas Müller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith},
      year={2025},
      eprint={2506.15742},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2506.15742}, 
}

@misc{trellis,
      title={Structured 3D Latents for Scalable and Versatile 3D Generation}, 
      author={Jianfeng Xiang and Zelong Lv and Sicheng Xu and Yu Deng and Ruicheng Wang and Bowen Zhang and Dong Chen and Xin Tong and Jiaolong Yang},
      year={2025},
      eprint={2412.01506},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.01506}, 
}
@misc{magiccolor,
      title={Follow-Your-Color: Multi-Instance Sketch Colorization}, 
      author={Yinhan Zhang and Yue Ma and Bingyuan Wang and Qifeng Chen and Zeyu Wang},
      year={2025},
      eprint={2503.16948},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.16948}, 
}
@misc{blora,
      title={Implicit Style-Content Separation using B-LoRA}, 
      author={Yarden Frenkel and Yael Vinker and Ariel Shamir and Daniel Cohen-Or},
      year={2024},
      eprint={2403.14572},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.14572}, 
}
@misc{yan2025imagereferencedsketchcolorization,
      title={Image Referenced Sketch Colorization Based on Animation Creation Workflow}, 
      author={Dingkun Yan and Xinrui Wang and Zhuoru Li and Suguru Saito and Yusuke Iwasawa and Yutaka Matsuo and Jiaxian Guo},
      year={2025},
      eprint={2502.19937},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.19937}, 
}
@misc{qwenimage,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324},
}

@misc{backgroundremove,
      title={An Inpainting-Infused Pipeline for Attire and Background Replacement}, 
      author={Felipe Rodrigues Perche-Mahlow and André Felipe-Zanella and William Alberto Cruz-Castañeda and Marcellus Amadeus},
      year={2024},
      eprint={2402.03501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.03501}, 
}
@misc{gpt4v,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and etc. },
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{ati,
      title={ATI: Any Trajectory Instruction for Controllable Video Generation}, 
      author={Angtian Wang and Haibin Huang and Jacob Zhiyuan Fang and Yiding Yang and Chongyang Ma},
      year={2025},
      eprint={2505.22944},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.22944}, 
}

@article{paintsalter,
author = {Zhang, Lvmin and Yan, Chuan and Guo, Yuwei and Xing, Jinbo and Agrawala, Maneesh},
title = {Generating Past and Future in Digital Painting Processes},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3731160},
doi = {10.1145/3731160},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {127},
numpages = {13},
keywords = {digital painting, generative models, diffusion models}
}


@misc{swiftsketch,
      title={SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation}, 
      author={Ellie Arar and Yarden Frenkel and Daniel Cohen-Or and Ariel Shamir and Yael Vinker},
      year={2025},
      eprint={2502.08642},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.08642}, 
}
@misc{neutralstroke,
      title={Neural Strokes: Stylized Line Drawing of 3D Shapes}, 
      author={Difan Liu and Matthew Fisher and Aaron Hertzmann and Evangelos Kalogerakis},
      year={2021},
      eprint={2110.03900},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.03900}, 
}

@inproceedings{brooks2023instructpix2pix,
  title     = {InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author    = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023}
}

@inproceedings{haque2023instructnerf2nerf,
  title     = {Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions},
  author    = {Haque, A. and others},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2023}
}

@article{yu2023editdiffnerf,
  title   = {Editing 3D Neural Radiance Fields using 2D Diffusion Models},
  author  = {Yu, Lang and others},
  journal = {arXiv preprint arXiv:2306.09551},
  year    = {2023}
}

@article{chen2023gaussianeditor,
  title   = {GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting},
  author  = {Chen, Yiwen and others},
  journal = {arXiv preprint arXiv:2311.14521},
  year    = {2023}
}

@inproceedings{wang2024gaussianeditor,
  title     = {GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions},
  author    = {Wang, Junjie and Fang, Jiemin and Zhang, Xiaopeng and Xie, Lingxi and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024}
}

@article{poole2022dreamfusion,
  title   = {DreamFusion: Text-to-3D using 2D Diffusion},
  author  = {Poole, Ben and others},
  journal = {arXiv preprint arXiv:2209.14988},
  year    = {2022}
}

@article{lin2022magic3d,
  title   = {Magic3D: High-Resolution Text-to-3D Content Creation},
  author  = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and others},
  journal = {arXiv preprint arXiv:2211.10440},
  year    = {2022}
}

@inproceedings{wang2023prolificdreamer,
  title     = {ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation},
  author    = {Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2023}
}

@article{li2025voxhammer,
  title   = {VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space},
  author  = {Li, Lin and Huang, Zehuan and Feng, Haoran and Zhuang, Gengxiong and Chen, Rui and Guo, Chunchao and Sheng, Lu},
  journal = {arXiv preprint arXiv:2508.19247},
  year    = {2025}
}


@incollection{personal,
  title={Using the visual arts to expand personal creativity},
  author={Nadeau, Roberta},
  booktitle={Using the creative arts in therapy and healthcare},
  pages={49--71},
  year={2003},
  publisher={Routledge}
}

@article{edu, 
title={Shifting Canvases: The Evolution of Media and Techniques in Art Education with Generative AI}, volume={1}, url={https://journal.iistr.org/index.php/JARDIN/article/view/1248}, DOI={10.56741/jardin.v1i01.1248}, abstractNote={&amp;lt;p&amp;gt;Generative AI has revolutionized art education by introducing novel media and techniques,  from AI-generated art and generative design to interactive, co-creative workflows. This opinion-based article critically examines how tools like DALL·E, MidJourney, Artbreeder, and Runway ML transform artistic teaching and learning. Through a literature review, we identify pedagogical implications, democratization effects, and ethical considerations. Our results, presented in structured tables, highlight emerging opportunities and tensions. We argue that AI serves as both a disruptor and enhancer, necessitating updated curriculum frameworks that embrace creative agency and critical literacy.&amp;lt;/p&amp;gt;}, number={01}, journal={The Journal of Art and Design Innovation}, author={Arslan, Aysel and Probosiwi}, year={2025}, month={Jul.}, pages={51–59} }

@Inbook{flow,
author="Chemi, Tatiana",
editor="Harmat, L{\'a}szl{\'o}
and {\O}rsted Andersen, Frans
and Ull{\'e}n, Fredrik
and Wright, Jon
and Sadlo, Gaynor",
title="The Experience of Flow in Artistic Creation",
bookTitle="Flow Experience: Empirical Research and Applications",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="37--50",
abstract="In the present chapter I will describe and discuss the experience of flow occurring when professional artists create and learn. The unique findings are part of a larger study that looked at artistic creativity and aimed at describing the ways in which professional and widely recognised artists create, learn and organise their work. The methodological approach was qualitative and based on retrospective narratives of 22 high-achieving professional artists. When asked to describe the ways in which they create, these artists often replied by mentioning a positively felt state of deep concentration and calm. Flow, to them, seems to be at the same time a pre-requisite, a constitutive element and an effect of creative processes. Artistic creation happens in a state of deep concentration and self-forgetting and flow seems to have a specific purpose within artistic processes: triggering, facilitating and guiding the flow of creation.",
isbn="978-3-319-28634-1",
doi="10.1007/978-3-319-28634-1_3",
url="https://doi.org/10.1007/978-3-319-28634-1_3"
}



@article{multivariate,
title = {How artists create: Creative process and multivariate factors},
journal = {Learning and Individual Differences},
volume = {26},
pages = {161-170},
year = {2013},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2013.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1041608013000356},
author = {Marion Botella and Vlad Glaveanu and Franck Zenasni and Martin Storme and Nils Myszkowski and Marion Wolff and Todd Lubart},
keywords = {Creativity, Artists, Process, Semantic analysis, Activity theory},
abstract = {This study sought to identify the factors that artists consider important for their creativity and to reconstruct, from interviews, the stages of their creative activity. For this purpose, 27 interviews with professional artists were analyzed using a double approach. First, a quantitative analysis of interviews and associated self-report questionnaires was performed. Second, a qualitative coding grid was applied to a representative subset of the interviews to uncover stages of activity and the interaction between creator and the material and social world. Results are discussed according to the multivariate approach and in light of activity theory and its emphasis on situated, goal-directed and meaningful action. Findings concerning the creative process and the factors involved are finally considered with respect to teaching creativity and art.}
}

@article{thought,
author = {Catharine Patrick},
title = {Creative thought in Artists},
journal = {The Journal of Psychology},
volume = {4},
number = {1},
pages = {35--73},
year = {1937},
publisher = {Routledge},
doi = {10.1080/00223980.1937.9917525},
URL = { 
        https://doi.org/10.1080/00223980.1937.9917525
},
eprint = { 
        https://doi.org/10.1080/00223980.1937.9917525
}
}

@article{wallas,
  title={The art of thought},
  author={Wallas, G},
  journal={Jonathan Cape},
  year={1926}
}

@article{wallas_followup,
  title={Wallas’ four-stage model of the creative process: More than meets the eye?},
  author={Sadler-Smith, Eugene},
  journal={Creativity research journal},
  volume={27},
  number={4},
  pages={342--352},
  year={2015},
  publisher={Taylor \& Francis}
}

@book{geneplore,
  title={Creative cognition: Theory, research, and applications},
  author={Finke, Ronald A and Ward, Thomas B and Smith, Steven M},
  year={1996},
  publisher={MIT press}
}

@article{vision,
  title={The creative vision: A longitudinal study of problem finding in art},
  author={Getzels, Jacob W and Csikszentmihalyi, Mihaly},
  journal={(No Title)},
  year={1976}
}

@article{rhodes,
  title={An analysis of creativity},
  author={Rhodes, Mel},
  journal={The Phi delta kappan},
  volume={42},
  number={7},
  pages={305--310},
  year={1961},
  publisher={JSTOR}
}

@article{soi,
  title={Guilford's structure of intellect model: its relevance for the teacher preparation curriculum},
  author={Edwards, Reginald},
  journal={Curriculum Theory Network},
  volume={1},
  number={3},
  pages={47--64},
  year={1969},
  publisher={Taylor \& Francis}
}

@article{modeling,
  title={Modeling the creative process: A grounded theory analysis of creativity in the domain of art making},
  author={Mace, Mary-Anne and Ward, Tony},
  journal={Creativity research journal},
  volume={14},
  number={2},
  pages={179--192},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{solving,
  title={The figural problem solving and problem finding of professional and semiprofessional artists and nonartists},
  author={Kay, Sandra},
  journal={Creativity Research Journal},
  volume={4},
  number={3},
  pages={233--252},
  year={1991},
  publisher={Taylor \& Francis}
}

@article{modification,
  title={The process of art-making and creative expertise: An analysis of artists' process modification},
  author={Yokochi, Sawako and Okada, Takeshi},
  journal={The Journal of Creative Behavior},
  volume={55},
  number={2},
  pages={532--545},
  year={2021},
  publisher={Wiley Online Library}
}

@article{educational,
  title={Educational intervention and the development of young art students' talent and creativity},
  author={Rostan, Susan M},
  journal={The Journal of Creative Behavior},
  volume={39},
  number={4},
  pages={237--261},
  year={2005},
  publisher={Wiley Online Library}
}

@article{control,
  title={A phenomenology of artistic doing: Flow as embodied knowing in 2D and 3D professional artists},
  author={Banfield, Janet and Burgess, Mark},
  journal={Journal of Phenomenological Psychology},
  volume={44},
  number={1},
  pages={60--91},
  year={2013},
  publisher={Brill}
}

@article{ecological,
  title={A dynamic and ecological approach to the artistic creative process of arts students: An empirical contribution},
  author={Botella, Marion and Zenasni, Franck and Lubart, Todd},
  journal={Empirical studies of the arts},
  volume={29},
  number={1},
  pages={17--38},
  year={2011},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{subproccess,
  title={Creative Subprocess Frequencies and Their Relation to Personal Characteristics and Product Creativity: Insights from a Drawing Task Think Aloud Study},
  author={Boldt, Gregory T and Kaufman, James C},
  journal={The Journal of Creative Behavior},
  volume={59},
  number={1},
  pages={e629},
  year={2025},
  publisher={Wiley Online Library}
}

@article{mfa,
  title={How artists create: An empirical study of MFA painting students},
  author={Sawyer, R Keith},
  journal={The Journal of Creative Behavior},
  volume={52},
  number={2},
  pages={127--141},
  year={2018},
  publisher={Wiley Online Library}
}

@article{osborn,
  title={Applied imagination 3”’Edition},
  author={Osborn, AF},
  journal={New York: Charles Scribner},
  year={1963}
}

@article{busse,
  title={Theories of the creative process: a review and a perspective.},
  author={Busse, Thomas V and Mansfield, Richard S},
  journal={The Journal of Creative Behavior},
  year={1980},
  publisher={Creative Education Foundation}
}


@book{encyclopedia,
  title={Encyclopedia of creativity},
  author={Runco, Mark A and Pritzker, Steven R},
  year={2020},
  publisher={Academic press}
}

@book{torrance,
  title={Guiding creative talent},
  author={Torrance, E Paul},
  year={2018},
  publisher={Pickle Partners Publishing}
}

@book{bruford,
  title={Making it Work: Creative music performance and the Western kit drummer},
  author={Bruford, William},
  year={2016},
  publisher={University of Surrey (United Kingdom)}
}

@article{furst,
  title={The creative process in visual art: A longitudinal multivariate study},
  author={F{\"u}rst, Guillaume and Ghisletta, Paolo and Lubart, Todd},
  journal={Creativity Research Journal},
  volume={24},
  number={4},
  pages={283--295},
  year={2012},
  publisher={Taylor \& Francis}
}

@book{boden,
  title={The creative mind: Myths and mechanisms},
  author={Boden, Margaret A},
  year={2004},
  publisher={Routledge}
}

@inproceedings{centric,
author = {Jacobs, Jennifer and Brandt, Joel and Mech, Radom\'{\i}r and Resnick, Mitchel},
title = {Extending Manual Drawing Practices with Artist-Centric Programming Tools},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174164},
doi = {10.1145/3173574.3174164},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {generative art, procedural art, programming},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{PortraitSketch,
author = {Xie, Jun and Hertzmann, Aaron and Li, Wilmot and Winnem\"{o}ller, Holger},
title = {PortraitSketch: face sketching assistance for novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647399},
doi = {10.1145/2642918.2647399},
abstract = {We present PortraitSketch, an interactive drawing system that helps novices create pleasing, recognizable face sketches without requiring prior artistic training. As the user traces over a source portrait photograph, PortraitSketch automatically adjusts the geometry and stroke parameters (thickness, opacity, etc.) to improve the aesthetic quality of the sketch. We present algorithms for adjusting both outlines and shading strokes based on important features of the underlying source image. In contrast to automatic stylization systems, PortraitSketch is designed to encourage a sense of ownership and accomplishment in the user. To this end, all adjustments are performed in real-time, and the user ends up directly drawing all strokes on the canvas. The findings from our user study suggest that users prefer drawing with some automatic assistance, thereby producing better drawings, and that assistance does not decrease the perceived level of involvement in the creative process.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {407–417},
numpages = {11},
keywords = {creativity support tool, novices, portraits, sketching},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{k_sketch,
author = {Davis, Richard C. and Colwell, Brien and Landay, James A.},
title = {K-sketch: a 'kinetic' sketch pad for novice animators},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357122},
doi = {10.1145/1357054.1357122},
abstract = {Because most animation tools are complex and time-consuming to learn and use, most animations today are created by experts. To help novices create a wide range of animations quickly, we have developed a general-purpose, informal, 2D animation sketching system called K-Sketch. Field studies investigating the needs of animators and would-be animators helped us collect a library of usage scenarios for our tool. A novel optimization technique enabled us to design an interface that is simultaneously fast, simple, and powerful. The result is a pen-based system that relies on users' intuitive sense of space and time while still supporting a wide range of uses. In a laboratory experiment that compared K-Sketch to a more formal animation tool (PowerPoint), participants worked three times faster, needed half the learning time, and had significantly lower cognitive load with K-Sketch.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {413–422},
numpages = {10},
keywords = {sketching, pen-based user interfaces, informal user interfaces, animation},
location = {Florence, Italy},
series = {CHI '08}
}

@inproceedings{bob,
author = {Benedetti, Luca and Winnem\"{o}ller, Holger and Corsini, Massimiliano and Scopigno, Roberto},
title = {Painting with Bob: assisted creativity for novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647415},
doi = {10.1145/2642918.2647415},
abstract = {Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {419–428},
numpages = {10},
keywords = {creativity, novices, painting},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}
\section{Phase 2 (Workshop) Findings}

In the following section, we present findings from our co-design workshops. We first report participants' reflections on their interactions with our prototypes and with the existing capabilities of relevant tools. We then describe new capabilities that participants surfaced through the workshops and the ways these expand the design space. Finally, we synthesize cross-cutting design tensions that span the emerging taxonomy of capabilities, highlighting trade-offs that matter for future design and deployment.

\subsection{Reference, Capture, 3D-Staging} \label{reference}

\subsubsection{Semantic Search and Reference Organization.}
\label{semantic}

Participants sought tools for semantic search over large image and video sets, returning per frame matches and automatically organizing and deduplicating reference material. We did not provide a prototype, and results are based on participants’ discussions and comparisons to state-of-the-art tools~\cite{VSC, lin2014visual, OrganizingPhoto}\aq{we can remove this sentence if we already said this earlier in methods, but we should keep the citations here}.
Several participants described ideal use cases for tools that \textbf{search through videos}~\cite{GenQuery, VSC}, expressing desires to use them for the early stages of visual art-making. For example, P10 explained potential benefits of performing an \textit{``\textbf{object search}''} on full videos like they have done for singular images (P10). Additionally, P11 elaborated that a tool such as \textit{Textual Search Video Frame} ~\cite{lin2014visual} could help them do real-time multi-person tracking for interactive art projects that involve body movement (P11). In short, participants described video-level search and frame-level retrieval as scaffolding for early ideation and blocking, which reduces the time spent hunting for references and enables faster iteration on pose and motion.

For a tool such as \textit{Organizing Image Collection}~\cite{OrganizingPhoto}, participants such as P13 expressed interest but noted that their personal photo libraries were not large enough to benefit: \textit{``if it were like a better Google of like finding references, that would be great. But if it were out of my own references, I just don't think I take that many photos''} (P13). Instead, P13 explained that they wanted these functions for the reference images they save from Google Search or other sources online instead. 
Together, the findings suggest that semantic search and \textbf{automatic organization of reference libraries} would materially lower the cost of finding poses and motions during early stages for artists.


\subsubsection{Pose and Camera Control with On-demand Reference Capture.}
\label{pose}
Participants prioritized fine-grained pose and camera control, with on-demand reference capture and structure-preserving remix. They used Interactive 3D to explore complete angles and perspectives while comparing it to model-driven pose and view edits in prior tools~\cite{wu2025qwen}. Reflecting on their experiences with our \textit{Interactive3D} prototype, artists described 3D models as helpful for \textbf{viewing a subject from a specific perspective} (P11, 13). However, they also raised concerns about fidelity: \textit{``messed up anatomy, or [other] messed up things that are not true reality---I'm afraid of that lessening my knowledge of the real world''} (P13). In short, participants valued \textbf{direct camera and pose control} in 3D as a dependable way to generate accurate references with trustworthy anatomical fidelity.

Beyond seeing its value for perspective referencing, participants also proposed concrete improvements to \textit{Interactive3D}. 
P1 wanted to change skin textures to make a model look \textit{``younger or older, or rougher or smoother''}, P2 desired fine-grained control of a model (e.g., manipulating precise finger and wrist poses) while P5 suggested integration with existing workflows (e.g., model download as an OBJ file). Taken together, these point to complementary workflows: (1) \textbf{study mode} with deep control and export for detailed reference building, and (2) \textbf{quick capture mode} that limits controls for speed and focus.

% Participants also reflected on the benefits of 
Other state-of-the-art tools (e.g., \textit{Qwen Image}) also offered useful features such as changing poses and perspectives~\cite{wu2025qwen}. For example, P9 valued support to change perspectives in 2D art because it shows new angles quickly. In general, participants prioritized \textbf{speed and convenience} for early ideation stages when they needed rapid alternative views without manual setup. In short, model-driven pose and view edits offered fast suggestions for exploration, while \textit{Interactive3D} provided grounded control for precise reference generation.


\subsubsection{3D-aware Editing of Complex Scene Changes.}
\label{aware}
Our analysis shows that artists sought tools capable of 3D-aware editing to change scenes in references. Participants reflected on use cases of our \textit{Interactive3D} prototype for \textbf{editing complex backgrounds} as well as state-of-the art tools such as \textit{VoxHammer}~\cite{li2025voxhammer}. For example, P10 explained that \textit{``if you're drawing a manga, and you need the city to be your background, then you can have a 3D-model and trace in the background''}. Moreover, P11 emphasized that that such 3D models for references are most useful when they \textit{``cannot picture the scene in [their] mind and [they cannot find the real scene in the world''}. These reflections position 3D-aware background editing as beneficial to scaffold early composition and scene continuity in cases where references may otherwise be less accessible to participants. 

Additionally, participants provided suggestions for improving 3D editing tools to fit their workflows. Some felt overwhelmed by the capabilities of 3D editing, mainly wanting the capability to make smaller changes. P8 explained that \textit{``I haven't used Blender. I haven't used ZBrush. So would find something like 3D editing of a model to be difficult if it's a whole scene''}. P12 expressed a similar desire to apply such tools for simplistic changes, such as in lighting a scene. In contrast, P11 wanted specific controls when editing such as the ability to edit a specific part of a scene while being able to lock the rest of the model. These concerns suggest benefits in gentle entry points and per object, region based controls that favor small, reliable edits like lighting or camera tweaks over full scene overhauls. 

\subsection{Structure-to-Style Refinement}
\label{s2s}

\subsubsection{Layered and Simplified Decomposition with Focus Control.}
\label{decompose}
%Artists want tools that split images into meaningful layers and simple shape block-outs, show construction lines, and add pose abstraction (stick-figure/gesture) with step-by-step anatomy layers (skeleton to muscle to planes to shading); should support structure preserving views and edits so they can isolate or mute elements, prioritize focal planes, spotlight subjects, plan main forms and composition quickly, and study underlying anatomy/structure quickly without redrawing.
%Our Tool: Nothing!
%Prior Lit Tool: Qwen-Image (Layer Break Down), SwiftSketch, Neural Strokes

Participants also expressed interest in tools for breaking references into meaningful layers and controlling what to focus on: layered, structure preserving simplification with simple toggles for focus~\cite{swiftsketch, neutralstroke, wu2025qwen}. 
% Several participants reflected on instances where such tools would be useful within their early-stage workflows. 
For example, P9 highlighted a \textit{blockout} view as especially useful for assisting them in reading landscape forms (P9)~\cite{Blockout3D}\todo{add blockout citation}. Others noted the use of \textit{SwiftSketch} for reducing complex forms such as robots into simpler blocks (P4 and P10). Overall, these accounts point to imagined tool uses for layered decomposition, supporting exploration in early-stages.

We also surfaced pitfalls and changes needed for tools. Some disliked filter style simplifications that merely restyle images \textit{``when it asks it to be more simple it looks like a pencil sketch filter''} and qualitative prompts such as \textit{simple} that do not map to consistent levels of abstraction (P10). These participants asked for progressive breakdowns of references with features such as \textbf{structure-preserving edits, automatic pose abstractions}, and numeric controls such as \textbf{stroke count} (P10). Across layers, several participants also expressed desires to \textbf{mute or spotlight subjects}, and \textbf{plan major forms and composition} quickly (P8, P9, and P10)~\cite{neutralstroke}. Moreover, P8 wanted a tool that took a values-first view, \textit{``think[ing] in areas of dark and light \dots rather than objects,''} run alongside object-aware layers to avoid color filter distraction (P8). These needs point to the potential of tools that can help participants break references into meaningful layers and control focus within references.

% Our analysis shows that artist needs for layered, anatomy-aware scene decomposition with fine-grained focus control, tools that can peel an image from big value masses to simple block-outs, construction lines, pose abstractions, and then back up to muscles/planes/shading, while letting them isolate or mute elements and prioritize focal planes without redrawing. Although we did not show a prototype, participants evaluated the capability concept against their practice and current tools. They welcomed layered breakdowns that go beyond silhouettes to reveal internal flow: “the outline…still extends into the inner part of the drawing…you’re following the inner direction of your skeleton,” so “just doing the outline like the silhouette isn’t really enough” (P10). They disliked filter-like simplifications that merely restyle images (“when it asks it to be more simple, it…looks like a pencil-sketch filter”) and qualitative prompts (“simple/simpler”) that don’t map to consistent abstraction levels (P10). Artists want progressive processes that step through pose, muscles, planes, shading with true structure-preserving edits, automatic pose abstractions such as stick figures and gesture “to summarize the pose into a more understandable form,” and use numeric controls for abstraction (e.g., stroke count, line density, shading level) so simplification is predictable and repeatable (P10). For 2D study, they also asked for a values-first view that “thinks in areas of dark and light…rather than objects,” run alongside object-aware layers to avoid “candy filter” distraction (P8). Across all layers, they want to toggle and mute parts, spotlight subjects, and quickly plan major forms and composition.

% Participants also reflected on previous research tools and idealized use cases to surface their desired requirements. P4 praised research prototypes like Swift Sketch for turning complex forms (e.g., robots) into “a few blocks…to push the forms…very exaggerated.” P9 highlighted a “blockout” view as especially useful for 3D, simple, unshaded shapes to read landscape forms and plan what to model, while doubting the same layer feature would help 2D as much.  Taken together, the feedback points to a simple multi-track setup with anatomy and gesture layers such as stick figures, construction lines, and muscle or plane breakdowns; form layers that provide block-outs and basic volumes; and value layers that display large light and dark areas followed by mid-tones. All of these should be real, structure-preserving layers that users can toggle and use to control focus.

\subsubsection{Detailed Zoom, Pattern Extraction, and Transparent Adjustments.}
\label{zoom}
%Artists want tools with crisp zoom, resolution-enhancement, and texture inspection to zoom into a style to grab its parts (textures, patterns, color palettes) and extract motifs to support having controlled variations; controls should be transparent for local/global changes (e.g., brightness, saturation, targeted color/patterns); should also be able to adjust a single attribute (line weight, shading level, hue/saturation, parts of objects/subjects) at a time.
%Our Tool: Visual Editing
%Prior Lit Tool: GenQuery, Qwen-Image (Detail Zoom-In, Increase/Decrease Resolution), MagicQuill
Participants wanted crisp zoom, resolution enhancement, and texture inspection that let them zoom into a style, isolate textures, patterns, and palettes, and make controlled variations with transparent local and global controls, including single-attribute adjustments. 

When testing our \textit{Visual Editing} tool, they praised the non-destructive history for safe exploration before touching the real piece, because it let them “trace your steps back or forward” (P3). They were frustrated when style controls felt vague or cosmetic. The ``pencil sketch'' option did not behave like real pencil and lines felt generic, and the tool handled photo fixes or collage effects better than preserving a specific drawing style (P8).
Edits sometimes ignored source style or color requests, for example asking for ``pink pastel'' produced neon or dark tones, and overall the tool often drifted into a different style than the original (P4). Participants also asked for fine control over faces and materials, indicating interests in structure-preserving when applying local changes to an image. They also noted upon zooming-in and extracting details for changes, tools should be intent-aware and edge-aware that goes beyond rigid selections and proactively clarifies ambiguous requests. For example, when a user says ``change shirt to hoodie,'' the tool should ask whether to replace the garment, add a graphic, recolor it, or adjust the fit (P10). 

Other tools helped participants round out this capability. P4 preferred outfit-focused detail zoom from \textit{Qwen-Image} for recreating historical clothing, asking for true pattern extraction so users can capture motifs for cosplay or pattern making. P8 praised \textit{GenQuery}-style controls that vary a single element inside a scene -- they also asked for a simple adjustment slider to study and change styles with clear controlled variation. P12 valued resolution upscaling to turn low-resolution assets into high-resolution references, as long as structure stays intact and details do not get invented. \jh{is invented the right word here?}

\subsubsection{Controllable Style Analysis, Transfer, and Combination.}
\label{style}
%Artists want tools that surface a style’s components (e.g., palette, texture, pattern), enable controlled mixing/remapping with previews, learn or mimic strokes via simple outlines guidance, and allow multiple-reference transferring/combining with explicit rules about what can change and what must stay fixed.
%Our Tool: Medium Transfer, Style Transfer, Style Combination
%Prior Lit Tool: GenQuery, B-LoRA, MagicColor
When learning a new style, participants asked for tools that decompose style parts such as technique, color, form, and stroke, then let them mix or remap those parts with live previews. They also desired to preserve structure when applying style transformations so edges and keypoints stay fixed, as well as the ability to learn strokes from simple outlines and rule sets that state what may change or stay put. 

With \textit{Medium Transfer} and \textit{Style Transfer}, participants reported how the tool helped them rethink linework and color choices and refine details. P1 used multiple examples to make the system learn ``key aspects'' of their style and subsequently embed them in a new image -- this process also surfaced insights about their own style, although current outputs still felt too generic. Others observed that the model often remapped colors without altering structure in useful ways but wanted stronger structure control. P12 asked for a \textbf{structure-preserving mode} and \textbf{style isolation} to exclude frames or backgrounds during style analysis, plus fine controls for colorfulness, brightness, and local composition so individual style components can be adjusted without affecting the whole image. 

For \textit{Style Combination}, participants considered it a way to learn new styles through controlled change and asked for two exploration paths. One is a \textbf{spread of quick variations for side-by-side comparison} when studying a style. The other is a \textbf{single precise output} when the target look is already clear. Participants additionally suggested readable diagnostics that turn results into study guides, such as palette and brush breakdowns tied to each output.

When manipulating styles, participants emphasized region-aware conrol to assign one style to the background and another to the subject, and guardrails for content exclusion and frame-to-frame consistency when fusing styles. For example, P4 mapped a pastel style onto a 3D character and got clean line art and a coherent palette. Image-to-image conversion kept pose for anatomy study (P10). A line-art-to-fantasy conversion captured the target aesthetic and would benefit from annotated palettes and brushes for analysis (P9). Participants used combinations to generate multiple exampels to reverse-engineer what works across outputs (P1), and noted that fixed structure with varying style is especially inspiring for study and practice (P11, P12). 

%P11 asked for region-aware assignment (e.g., one style for the background, another for the foreground), and P8 showed how unusual fusions (like \textit{Maus} with open impressionism) can yield metaphor-rich frames if guardrails exist for content exclusion and frame-by-frame consistency. Across these cases, P1 used combinations to generate multiple examples of a look, then reverse-engineer which specific elements were most appealing. P12 highlighted when character structure stays fixed and style varies, the tool becomes an effective source of inspiration.

\subsection{Transparency, Learning, Community}

\subsubsection{Explainable and Transparent Learning Workflows.}
\label{transparent}

%Artists want to pair in-tool guidance (clear instructions, guided prompts, step-by-step tutorials) with shareable provenance (view/share the exact steps, models, and settings) so artists learn how to pick and create strong/good references for new art topics/domains, understand why results look the way they do, and quickly remix or refine with better prompts or sketches inputs.
%Our Tool: Step-by-Step Tutorial
%Prior Lit Tool: PaintAlter
Participants expressed interests in tools that pair \textbf{in tool guidance} with \textbf{shareable provenance} so they can learn how to pick strong references, understand why results look the way they, and quickly remix or refine with better prompts or sketches. They want clear instructions, guided prompts, and step by step tutorials tied to exact steps, models, and settings that can be viewed, studied, and rerun.

Experiences with our \textit{Step-by-Step Tutorial} surfaced what to keep and what to change. P4 valued a staged breakdown that moves from flats to simple shading to color and texture, since it reveals how effects like gloss are built rather than guessed from a finished image. P1 asked for tutorials that specify the exact actions needed to reproduce another artist's coloring and noted that stimulating human mistakes is distracting in a tutorial. Others like P7 and P8 found that the current tool behavior too much like tracing or reveal by parts rather than "a natural process that humans will draw" that starts with gesture, construction, form and shading, then detailing (P12). They asked for a true \textbf{draft to final pipeline} that shows how features and color layers emerge in a natural order.

Participants asked for an explainable and tranpsarent learning flow that shows real making steps to understand cause and effect relationships. P8 said current tutorials like \textit{PaintsAlter} and \textit{Step-by-Step Tutorial} often jump from canvas to finished scene with limited intermediate steps; instead they want to see outlines appear, colors laid down in order, and details added gradually so they can answer ``what order did they lay the colors down.'' They also wanted a single place to pin references and move step by step, describing a workflow of ``I want to draw this, show me how.'' P4 emphasized tools should make intent legible and repeatable. Having a \textbf{scaffolded input box} improves results by letting participants specify exactly what they want, and in 3D they can apply the same logic to learn from pose and structure. Taken together, participants' requests point to guided prompts and staged tutorials that mirror human practice so artists can learn and stay in control.

\subsubsection{Community Spaces and Feedback Loops.}
\label{community}
%Artists want spaces for sharing of references, remix challenges, and reproducible pipelines (reuse others’ recipes); plus role-played AI critiques with optional human reviews to get both professional and social input; tie constructive feedback based on artists’ intent/goals and include privacy/consent controls.
%Our Tool: Nothing!
%Prior Lit Tool: CognArt
Participants asked for \textbf{lightweight community spaces} that support sharing references, running remix challenges, and reusing each other’s \textbf{plug-and-play pipelines}. They want \textbf{fast goal-aware feedback} from AI, optional human critique, and clear privacy and consent controls upon joining a community. Without a prototype, they reacted to the concept and pointed to gaps in current tools. P8 described a simple social layer where artists post references, view many remixes of the same prompt or image, and browser the exact steps behind a piece. They envisioned a \textbf{Hugging Face for artists} where creators publish pipelines with models, steps, settings, and notebooks that others can reuse or adapt. Capturing how pipelines are used would help artists and surface lesser know models that are hard to discover.

Participants highlighted adjacent needs that current platforms do not meet. P4 asked for an \textbf{up-to-date meme library} with searchable templates, recency signals, and links to original post since general search often returns stale content. They wanted the same for reference search so current and culturally relevant formats are easy to find and import straight into a working file or pipeline. P8 also pointed to \textbf{asset-first sharing} where people publish building blocks and explain how they were used to strengthen learning and reproducibility.

For critique, participants want a hybrid loop that respects intent. When shown the idea of \textit{CognArt} with analysis trends and feedback based on breakdown elements, P10 still favored real human critique: “If I have access to a real human, I would prefer a real human.” P11 proposed \textbf{role-played AI critics}, such as a gallery curator, animator, or hobbyist, to broaden perspectives when those voices are not available to artists on demand. P12 recommended combining both: use AI for fast, professional suggestions when improving or monetizing work, seek human attention when audience building matters, and skip feedback entirely when creating just for fun. They noted that high quality human critique is hard to get, which is where \textbf{structured objective AI guidance} can help, with the option to schedule human reviews when needed.

% \subsubsection{Design Tensions for Feedback Capabilities.}

\subsection{Design Tensions}

% to backreference capabilities:

% Semantic Search and Reference Organization \ref{semantic}

% Pose and Camera Control with On-demand Reference Capture \ref{pose}

% 3D-aware Editing of Complex Scene Changes
% \ref{aware}

% Layered and Simplified Decomposition with Focus Control
% \ref{decompose}

% Detailed Zoom, Pattern Extraction, and Transparent Adjustments \ref{zoom}

% Controllable Style Analysis, Transfer, and Combination \ref{style}

% Explainable and Transparent Learning Workflows \ref{transparent}

% Community Spaces and Feedback Loops \ref{community}



In this section, we highlight tensions that emerged throughout participants' reflections on capabilities in the co-design workshops. 

\subsubsection{Assistance vs Independence.} 
A major tension emerged around how much assistance models should provide while still preserving novice artists agency and workflow. Participants welcomed tools that could teach and scaffold within the early-stages of their workflow, but insisted that such guidance should not steer their work. P8 wanted live \textit{``what this looks like''} matches based on art history and pop-culture to build taste and test how others might read their work, but warned that real-time labeling risks boxing them in byimporting biased and clichéd references, discouraging originality, and breaking creative flow (P8). P8 suggested offering multiple procedural paths with a simple timeline to switch among them, prompts such as \textit{``what else could go here?''}) to invite abstraction, and light intent checking (\textit{``here’s how I read your prompt, here is how my model works''}) so agency remains with the artist(P8).
% Another design tension emerged between models that helped scaffold the artistic process into individual steps versus enabling artists to freely explore for themselves. Specifically, artists commented that tutorials and AI guidance that split work into scaffolded steps can help with building confidence and learning, but argued that such a constrained setup may hurt independence. For example, 

Participants also clarified where feedback belongs and how to deliver it without taking over. For example, P13 advised that tools should not \textit{``take the fun out of art''}. P10 proposed a division of labor for feedback:
\begin{quote}
    \textit{``since artists want feedback yet fear AI feedback being generic and nonconstructive, perhaps the ideal case would be to have AI be good at giving feedback for \textbf{ground-truth domains} with expert rubrics (e.g., anatomy) but avoid \textbf{prescriptive judgments} for subjective art and self expression (e.g., when helping with style or color palettes without standard definitions)''}
\end{quote}
Non destructive options such as temporary overlays or side-by-side-comparisons can keep artists original work intact while building on suggestions. Overall participants advocated for tooling that assisted them rather than make decisions for them, providing adjustable guidance under their control that improves craft while preserving flow. 

\subsubsection{Unexpected Surprise versus Accuracy.} 
Another concern was with how much to tools should welcome surprise versus enforce accuracy. For many participants, serendipity was valuable for inspiration. P1 accidentally swapped the order of the style combination input, but was surprised to be happpy with the output, noting they liked the \textit{``hard shadows''} and \textit{``fuzzy neon highlights''}. Further, P2 noted that failed outputs may be \textit{``more helpful than the successful ones''} since it could be weirder or more unexpected in a way that provides them with inspiration. In these cases, we found that random and vague outputs were useful for open-ended brainstorming especially when borrowing elements rather than replicating them. In contrast, for committed work, participants emphasized the importance of fidelity and control. In this way, P2 cautioned that current systems are better for ideation than for early sketches. P1 and P3 expressed a more optimistic option, citing that tools such as our \textit{Interactive3D}prototype could be helpful for pose changes of references if only the lighting, shadows, and model proportions were close to real life. 

Moreover, participants also weighted realism against the importance of pieces having a human touch to them. P7 explained how the generated image in style transfer \textit{``looks so good''} but also felt like \textit{``AI art''}. P2 wanted integrated AI tools (in software like Procreate/Photoshop) that improved their workflow by fixing proportions and lighting, or refining their style, but without generic outputs that look \textit{``too AI''}. Overall, participants wanted controlled instances of surprise, in which tools could invite unexpected variation during exploration while preserving precise, non-generic and human-guided results as their work moved toward completion. 

% Overall, artists noted that random, vague, and unexpected outputs are great for divergent and open-ended brainstorming. The unexpected nature of certain AI tools also provide for fun inspiration, especially when artists are borrowing elements and not replicating them. However, artists highlighted that real work demands accurate fidelity and control - P2 cautioned that AI may be more helpful to generate ideas given its randomness rather than helping with early-stage sketches at its current performance level. P1 and P3 found the pose change and interactive 3D capabilities helpful, but noted they value accuracy so everything including shadows, lighting, and proportions need to be close to real life.

% A similar tension emerged between generated content that look too realistic vs those that are slightly imperfect, but retaining the `human touch'. For example, both P1 and P7 complained that a lot of the generated output from the prototypes (e.g., style transfer) looked very ``AI'' in that they had to regenerate several times to get a non-generic output. P7 explained how the generated image in style transfer \textit{``looks so good''} but also felt like \textit{``AI art''}. P2 wanted integrated AI tools (in software like Procreate/Photoshop) that improved their workflow by fixing proportions and lighting, or refining their style, but without generic outputs that look \textit{``too AI''}.

\subsubsection{Values versus Concerns of AI.} 
There was also tension between artists, who despite seeing the value of AI as tools, are also aware of their potential in exploiting artists like themselves. Participants saw value in tools that took advantage of AI capabilities, but hestitated to trust them. P3 found it difficult to use AI tools to help improve their art because the even the act of uploading their art means that AI could plagiarize it. Moreover, P1 said that they would not unless contributing artists had granted permission for the use of their work. P5 further noted that they would feel comfortable only once artists were fairly compensated as well. As such, we found that participants tied trust to practices of consent and transparency, advocating for AI that was trained on public domain or opt-in licensed data and payment to artists. 

Furthermore, participants wanted to be able to teach AI systems without copying the output. The reasoning behind this stemmed from a view of style as personal characteristic that some participants compared to handwriting (e.g., P10). P1 argued that \textit{``drawing a reference generated exactly from AI is the same as copying someone else's art and that you shouldn't be able to claim that as your own work, but you can copy as more of a learning process''} (P1). P8 advocated for turning tools into learning aids to help artists draw their own icons, for example, rather than copying the words of others (P8). Overall, participants strongly advocated for consent-based, transparent tools that could help novice artists such as themselves build skills and provide guidance while avoiding appropriation. 

% In general, artists want AI that helps them make better art, but they will not trust or upload work to tools trained on scraped datasets that may violate other artists' copyrights. Furthermore, trust can only be earned if tools use public domain or opt-in licensed data and pay artists, show where training images came from and who an output might echo (similar artist names), let users mark uploads with ``don’t train'' and keep them private, and offer curated/permission-based datasets.

% Finally, artists want AI to teach without copying, by treating each artist's style as personal (like one's own handwriting), focusing on techniques and moves (proportions, stroke economy, palette) rather than artist-name presets, keeping the original structure while applying style, and offering a gradual \textit{``blend into my own style''} transition. P1 explains that \textit{``drawing a reference generated exactly from AI is the same as copying someone else's art and that you shouldn't be able to claim that as your own work, but you can copy as more of a learning process''}. P8 even felt guilty using AI-made icons, and recommended making control sketch a learning tool that helps people draw their own icons instead of copying.


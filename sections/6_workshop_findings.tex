\subsection{Workshop Findings}


\subsubsection{Capability Stance (what to embrace, bound, or avoid)}
• Capabilities participants want (and why)
• Capabilities participants use only with X safeguards or Y contexts
• Capabilities they don’t want available by default (and why)

\subsubsection{New Considerations}
• Prototype-specific (issues/insights tied to our 6 particular probes)
• GenAI-general (cross-tool cautions or reflections)


\subsubsection{New Capabilities}

\todo{paul is doing this - these are needs 7 8 9 not covered by the 6}

In addition to the co-design feedback on the presented 6 prototypes, participants shared new needs previously uncovered in \S\ref{interview}. We group these additional needs into 2 potential tools that should be designed by the AI community in the future to bring about impact in co-creation.

The first set of tools involve improved \textbf{visual databases}. These are data management systems that include large banks of visual information alongside how they are indexed, stored, and searched by various attributes. This stemmed from needs involving better reference management, creating one's own references, and better retrieving reference from diverse attributes in both 2D and 3D. For example, P9, P10, and P11 all recommended a better visual database for indexing and retrieving 3D assets for both single and multi-object selection, with P9 giving an example of performing quick object scans and viewing them in 3D so that artists don't need to search and buy props when using them as references.

A visual database can also lead to a sense of community, sharing, and reuse, a theme shared by several participants. P4 recommended an up-to-date library with searchable templates, trend recency, and links to the original post for each search, while P8 recommended a social layer as a place to share references, run remix challenges, and post plug-and-play pipelines and recipes. Both commented that the community where people can learn how others made things where equally, if not more, important than the final visual outputs.

Finally, there was a need to extend these visual databases for videos as well. Participants called for efficient video search and frame analysis tools help artists to quickly find, study, and use motion, reference, and context. P8, P10, and P11 all agreed that fine-grained video frame analysis tools could help with real-time multi-person tracking for interactive art involving body movement, such as dance, action scenes, or tracking pedestrians and wildlife. P10 additionally identified how use cases like text to video frame search, video organization, and video zoom features can directly support quick retrieval and close study with references, with potential impact in video recommendation.

The second set of tools involve \textbf{visual interpretations (or visual feedback?)} - simulating how another expert or novice artist perspective would react to the visuals created by the artist, as a form of feedback for learning and improvement. Despite this common need for feedback, participants differed in the type of feedback they valued - P10 preferred human critique instead of AI-generated feedback, suggesting to keep human mentors and reviewers in the loop, while P11 recommendation that AI feedback has the potential to broaden perspectives by offering role-played critics (e.g., gallery curator, animator, hobbyist), can be done at scale across multiple personas and in real time. Finally, P12 suggested a hybrid path where fast AI feedback can be combined with optional human sessions or reviews, where social attention can be obtained when audience-building matters, and when feedback can be skipped entirely when creating just for fun.

Another dimension of visual interpretations is the ability of system to be more explainable in providing feedback rather than providing appealing demos, so that users becoming more trusting and have actionable insights to improve. P4 suggested always including a ``details'' text box with specific instructions to make results better, and to make existing tools more interactive so people can play with them (e.g., adjust simplicity and line weight). P8 also identified the potential to combine many complementary prototypes over multiple steps to better provide visual feedback. At the same time, P8 also pointed out a potential design tension, wanting room for `happy accidents' that require the model not over-explaining limits, providing too much feedback, and protecting the privacy of their local art and ideas.

% P10 - users thought styles are opaque and unpredictable (did blending instead of “apply B to A,” odd strength effects turned art into more human-like), so the tool UI should clarify the mental model, preview style mixing, and let users constrain what can/can’t change
% P10 - sometimes outlines depend on 3D understanding; the tool should be able to teach form-first sketching and visualize underlying volumes or muscles
% P12 - targeted recoloring with text worked, and global prompts can increase brightness/saturation for images; tension - user wants more control over specific factors or parts with more adjustment support; current support is not transparent to user




Style exploration and control
Guided learning, documentation, and memory

Perspective and composition tools


Step by step
- Shape, structure, and layer decomposition


Pose, gesture, and anatomy abstraction

\textbf{\textit{Fig 2. Final Taxonomy v2 after workshops (what changed and why)}}

Discussion - Connection to design framework of how v2 translates into actionable design implications for future genAI tools


% agency/control


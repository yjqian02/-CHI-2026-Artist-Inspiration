\subsection{Interactive Co-design Probes} \label{probes}
% \jc{add motivations (table): needs $\rightarrow$ prototypes}
\begin{figure}[tbp]
    \centering
    \resizebox{1\textwidth}{!}
    {\includegraphics{figures/Prototypes.pdf}}
    \caption{\textbf{Development of Co-design Probes.} Six interactive prototypes were developed to address needs identified through user interviews and to explore AI-assisted design workflows, belonging to three broad categories. (1) Structure-to-style refinement tools help artists progressively convert initial ``structured'' forms (e.g., a basic shape, outline, style, or geometric layout) into more aesthetically complex or stylized versions, such as visual editing, style transfer, and medium transfer. (2) Reference, capture, and 3D-staging tools help search, store, and combine references to source material (e.g., style combination) in both 2D and multi-view 3D environments (e.g., interactive 3D creation from a single image). (3) Transparency, learning, and community-based tools help make the most salient parts of the input more transparent, such as step-by-step tutorials of visual content to help artists learn new techniques and styles.}
    \label{fig:prototypes}
\end{figure}

Based on novice artist challenges identified above, we developed interactive co-design probes that (1) responds to specific challenges while also (2) showcasing relevant state-of-the-art generative  AI capabilities. These tools fulfilled three broader categories: (1) reference, capture, 3D-staging, (2) structure-to-style refinement, and (3) transparency, learning and community. While we presented them in isolation, participants were free to use them in any combination with one another, potentially across multiple steps to achieve more desired effects.

% \todo{we need to recategorize these. also make clear what is from literature and what is new from us. change to novice artists.}

\subsubsection{Structure-to-Style Refinement} tools progressively convert initial ``structured'' forms (e.g., basic shape, outline, style, or geometric layout) into more aesthetically complex or stylized versions. 
% We find that the following tools for structure and style refinement - visual editing, style transfer, and medium transfer, can help address key needs raised by novice artists. 

\paragraph{(i) \underline{Visual editing}. } Traditional image editing with generative models is largely text-driven, but artists often find text prompts insufficient for capturing precise edits. We leveraged MagicQuill~\citep{liu2025magicquill} to combines direct visual manipulation with generative editing, allowing users to sketch or mark directly on an existing image (need C2.1), reducing the burden of linguistic description while affording immediate, tangible control.
% over how images are 
% , such as drawing where a new element should be inserted, erasing unwanted objects, or adjusting color regions, while also supporting optional text instructions to refine intent (e.g., ``make the fish's mouth bigger''). This approach transformed.

\paragraph{(ii) \underline{Style transfer}.}
% Designers often seek to apply the stylistic qualities of one work to the content of another \todo{paul - be more precise and quote the specific needs}.

To match artists' desires to observe other's work and translate others' stylistic tradition to their own (C2.2), 
% developed a prototype that enables the transfer of visual style between images. Building on recent 
% advances in generative editing, 
we adapted FLUX Style Shaping\footnote{https://huggingface.co/spaces/multimodalart/flux-style-shaping}, a recent advance in generative editing based on ~\citep{flux} to enable flexible restyling while preserving image fidelity. 
% In our prototype, users provided two inputs: a 
Users provide (1) a \textit{structure image} defining the underlying composition, and a (2) \textit{reference image} to apply style onto. 
The system further contain controls to adjust balance between semantic preservation and stylistic strength.
% , affording fine-grained control over balance of original structure versus strength of reference style. 
% This design showcased the creative potential of style transfer as a way to reimagine existing imagery and expand a designer’s stylistic repertoire.

\paragraph{(iii) \underline{Medium transfer}.}
Also based on C2.2, artists often wonder how a reference image might appear in a different medium --- e.g., \textit{a cat in oil paint re-rendered as a pencil sketch or watercolor}. 
% Rather than starting from scratch, they often prefer to preserve an image’s visual content while reinterpreting it through another material form. 
% To address this, w
We built an interface that used the GPT-image-1 editing model to transform user's image into their target medium.
% of choice, for example, \textit{a cat in oil paint re-rendered as a pencil sketch or watercolor}. 
% Participants uploaded an image, specified a target medium, and received a transformation generated with the GPT-image-1 editing model. This prototype allowed us to test how medium variation can inspire new creative workflows while maintaining the original semantics of the subject.

\subsubsection{Reference, Capture, and 3D-staging.} 
These tools help search, store, and manage references to source material like images, videos, texts, or styles.
% . A key need from novice artists is the ability to flexibly combine multiple references to form a more coherent image (i.e., style combination). 
In addition to 2D source material and references, some tools in this category also extend to multi-view 3D environments.
% , such as animating images through time or across multiple views (i.e., interactive 3D creation from a single image).

\paragraph{(i) \underline{Style Combination}.}
Artists often draw on multiple references simultaneously, blending objects, motifs, or styles to spark unexpected ideas. A recurring challenge (C2.2) remains around merging elements into a single coherent composition — e.g., \textit{a realistic panda rendered in the aesthetic of a traditional Gyotaku fish print}.
% To simplify the process of translating photographs, styles, or poses into a unified artistic vision, we built a prototype 
Using GPT-Image-1~\cite{gpt4v}, we allowed artists to perform \textit{multi-reference combination} with a \textit{structure image} that defines the overall layout. 
% They then describe how the references should be integrated, enabling the system to generate a unified image that reflects combinations. This prototype shows how AI tools can help designers seamlessly merge disparate visual influences.
% Designers frequently draw on multiple references simultaneously, blending objects, motifs, or styles to spark unexpected ideas. A recurring challenge, however, lies in merging these diverse elements into a single coherent composition—for example, \textit{depicting a realistic panda rendered in the aesthetic of a traditional Gyotaku fish print}. To examine this need, we created a prototype that leverages GPT-Image-1~\cite{gpt4v} to support \textit{multi-reference combination}. Users could upload any number of \textit{reference images}—sources of styles, textures, or objects—along with a \textit{structure image} that defines the overall layout and composition. to define the overall composition. They were then asked to describe how styles and objects from the references should be integrated, enabling the system to generate a unified image that reflects these combinations. This prototype highlights how AI tools can help designers seamlessly merge disparate visual influences.

\paragraph{(ii) \underline{Interactive 3D editing}.}
% Some designers, particularly those working in 3D animation, want the ability to interactively manipulate meshes—for example, \textit{pulling the face of a 3D cat}. To explore this capability, we 
To combine 2D editing with 3D reconstruction, we again leveraged GPT-Image-1 to enables high-quality edits directly onto a 2D rendering. The modified image is then passed through Trellis~\citep{trellis}, to generate a corresponding 3D asset -- meeting C2.1.
% , allowing designers to see their edits materialize in three dimensions. This pipeline demonstrates how existing generative tools can be used to extend familiar 2D editing practices into interactive 3D design contexts.

\subsubsection{Transparency, Learning, and Community.} The final set help the creative and collaborative process among groups of human artists by making salient parts of inputs more transparent 
% such as highlighting visual information, obtaining simpler sketches, 
or summarizing the image using its key attributes. 
% Transparent tools can also help learning and community building, through summarizing step-by-step tutorials of visual content to help artists learn new techniques and styles, and cultivating a broader ecosystem of open-source tools, feedback loops, and shared knowledge.

\paragraph{(i) \underline{Step-by-step tutorial}.} 
% \textit{``I want a step-by-step breakdown of how this cat painting was made.''} This type of request reflects designers’ desire not only to access finished works, but also to understand the process behind them \todo{paul - be more precise and quote the specific needs}. 
To support learning art technique and style from existing work
% and make the learning process more transparent
(C4.2), we built upon PaintsUndo\footnote{https://github.com/lllyasviel/Paints-UNDO} to prototype the generation of process-oriented tutorials videos from a static image to visualize intermediate steps of art making.
% Built on the PaintsUndo\footnote{https://github.com/lllyasviel/Paints-UNDO} project, the system disentangles a completed image into distinct stages of creation—such as sketching, inking, coloring and shading—and then animates these stages into a sequential video. By visualizing the intermediate steps, this prototype serves as a medium for teaching techniques and supports skill development.

% no commercially available tools for needs 4.2-4.4, but they did have access to google search, etc.

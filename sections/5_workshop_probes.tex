\subsection{Development of Co-design Probes} \label{probes}
% \jc{add motivations (table): needs $\rightarrow$ prototypes}
\begin{figure}[tbp]
    \centering
    \resizebox{1\textwidth}{!}
    {\includegraphics{figures/Prototypes.pdf}}
    \caption{\textbf{Development of Co-design Probes.} Six interactive prototypes developed to explore AI-assisted design workflows:  (1) Visual Editing for direct manipulation with generative tools, and (2) Interactive 3D for extending 2D edits into three-dimensional space. (3) Step-by-Step Tutorial for generating process breakdowns from finished works. (4) Style Transfer for applying a qualities between images, (5)Medium Transfer for transforming images across artistic media, (6) Style Combination for merging multiple visual references. Each prototype addresses specific designer needs identified through user interviews.}
    \label{fig:prototypes}
\end{figure}

Based on the needs identified through our interviews, we developed co-design probes. Each prototype responds to specific needs that emerged from our user research while demonstrating the potential for AI tools to augment creative workflows. At a high level, these tools enable users to summarize, translate and create visual content categorized based on whether they reduce, maintain, or expand information content in a controllable manner.


\textbf{\textit{Fig 1. Initial Taxonomy v1 after interviews (original needs/what participant mentioned/with lit examples of possible capabilities/considerations)}}


\subsubsection{Translation} approaches modify or edit information in certain dimensions while keeping other desired attributes consistent. We find that the following translation-based tools: visual editing, style transfer, and medium transfer, can help address key needs raised by novice artists. 

\paragraph{(i) \underline{Visual editing}. } Traditional image editing with generative models is largely text-driven, requiring users to describe desired changes in words. Yet designers often find text prompts insufficient for capturing precise edits, particularly when they involve subtle spatial adjustments or specific visual details. To address this limitation, we use MagicQuill~\citep{liu2025magicquill}, an integrated image editing system that combines direct visual manipulation with generative editing. The interface allows users to sketch or mark directly on an existing image, such as drawing where a new element should be inserted, erasing unwanted objects, or adjusting color regions, while also supporting optional text instructions to refine intent (e.g., ``make the fish's mouth bigger''). This approach reduces the burden of linguistic description and gives designers more immediate, tangible control over how images are transformed.

\paragraph{(ii) \underline{Style transfer}.}
% Designers often seek to apply the stylistic qualities of one work to the content of another \todo{paul - be more precise and quote the specific needs}.

To bridge the gap between observing other's work and translating from others' stylistic tradition to their own, we developed a prototype that enables the transfer of visual style between images. Building on recent advances in generative editing, we adapted FLUX Style Shaping\footnote{https://huggingface.co/spaces/multimodalart/flux-style-shaping}, a method based on FLUX.1 Redux~\citep{flux} that allows flexible restyling while preserving image fidelity. In our prototype, users provided two inputs: a \textit{structure image} that defined the underlying composition, and a \textit{reference image} whose style would be applied. The system further exposed controls for adjusting the balance between semantic preservation and stylistic strength, giving participants the ability to fine-tune how much of the original structure was retained versus how strongly the reference style was imposed. This design showcased the creative potential of style transfer as a way to reimagine existing imagery and expand a designer’s stylistic repertoire.


\paragraph{(iii) \underline{Medium transfer}.}
A common challenge designers face is exploring how a reference image might appear in different artistic media. Rather than starting from scratch, they often prefer to preserve an image’s visual content while reinterpreting it through another material form. To address this, we built a prototype that transforms a user-provided image into a medium of choice—for example, \textit{a cat in oil paint re-rendered as a pencil sketch or watercolor}. Participants uploaded an image, specified a target medium, and received a transformation generated with the GPT-image-1 editing model. This prototype allowed us to test how medium variation can inspire new directions while maintaining the original semantics of the subject.
% One challenge frequently raised by designers is the difficulty of exploring how a reference image might appear in different artistic media. Rather than beginning from scratch, they often prefer to retain the visual content of an image while reinterpreting it through another material form. To investigate this need, we created a prototype that transforms a user-provided reference image into a new medium of their choice. For instance, \textit{a cat depicted in oil paint could be re-rendered as a pencil sketch or watercolor}. Participants interacted with the system by uploading an image and specifying a desired medium, after which the transformation was generated using the GPT-image-1 editing model. This prototype offered a means of testing how medium variation could inspire new directions while preserving the original semantic of drawing matter.

\subsubsection{Summarization} approaches help identify one or more most salient parts of the input, such as highlighting visual information, obtaining simpler sketches, or summarizing the image using its key attributes. Notably, we include tools for developing summarized step-by-step tutorials of visual content which were a critical need identified by visual artists to learn new techniques and styles.

\paragraph{(i) \underline{Step-by-step tutorial}.} 
% \textit{``I want a step-by-step breakdown of how this cat painting was made.''} This type of request reflects designers’ desire not only to access finished works, but also to understand the process behind them \todo{paul - be more precise and quote the specific needs}. 
To support learning art technique and style from existing work and make the learning process more transparent, we developed a prototype that generates process-oriented tutorials videos from a static image. Built on the PaintsUndo\footnote{https://github.com/lllyasviel/Paints-UNDO} project, the system disentangles a completed image into distinct stages of creation—such as sketching, inking, coloring and shading—and then animates these stages into a sequential video. By visualizing the intermediate steps, this prototype serves as a medium for teaching techniques and supports skill development.

\subsubsection{Creation} extends visual information in a consistent way, often based on targeted sampling from generative models, such as animating images through time or across multiple views (i.e., interactive 3D creation from a single image), or combining multiple parts to form a more coherent image (i.e., style combination).

\paragraph{(i) \underline{Style Combination}.}
% Designers often draw on multiple references simultaneously, blending objects, motifs, or styles to spark unexpected ideas. A recurring challenge, however, is merging these diverse elements into a single coherent composition—for example, \textit{a realistic panda rendered in the aesthetic of a traditional Gyotaku fish print}. 
To simplify the process of translating photographs, styles, or poses into a unified artistic vision—for example, \textit{a realistic panda rendered in the aesthetic of a traditional Gyotaku fish print}—we built a prototype using GPT-Image-1~\cite{gpt4v} for \textit{multi-reference combination}. Users upload any number of \textit{reference images}—sources of styles, textures, or objects—along with a \textit{structure image} defining the overall layout. They then describe how the references should be integrated, enabling the system to generate a unified image that reflects combinations. This prototype shows how AI tools can help designers seamlessly merge disparate visual influences.
% Designers frequently draw on multiple references simultaneously, blending objects, motifs, or styles to spark unexpected ideas. A recurring challenge, however, lies in merging these diverse elements into a single coherent composition—for example, \textit{depicting a realistic panda rendered in the aesthetic of a traditional Gyotaku fish print}. To examine this need, we created a prototype that leverages GPT-Image-1~\cite{gpt4v} to support \textit{multi-reference combination}. Users could upload any number of \textit{reference images}—sources of styles, textures, or objects—along with a \textit{structure image} that defines the overall layout and composition. to define the overall composition. They were then asked to describe how styles and objects from the references should be integrated, enabling the system to generate a unified image that reflects these combinations. This prototype highlights how AI tools can help designers seamlessly merge disparate visual influences.

\paragraph{(ii) \underline{Interactive 3D editing}.}
Some designers, particularly those working in 3D animation, want the ability to interactively manipulate meshes—for example, \textit{pulling the face of a 3D cat}. To explore this capability, we built a prototype that combines 2D editing with 3D reconstruction. The workflow begins with GPT-Image-1, which enables high-quality edits directly on a 2D rendering of the object. The modified image is then passed through Trellis~\citep{trellis}, which takes a 2D image and generate a corresponding 3D asset, allowing designers to see their edits materialize in three dimensions. This pipeline demonstrates how existing generative tools can be used to extend familiar 2D editing practices into interactive 3D design contexts.

While we present these tools in isolation, there is also a potential to combine multiple tools together, potentially across muiltiple steps, to bring about better outcomes.. \todo{paul - should we say this?}